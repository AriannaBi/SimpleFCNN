{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "99038744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "import sys\n",
    "from functools import reduce\n",
    "\n",
    "# os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "# # random seed for numpy\n",
    "# np.random.seed(37)\n",
    "# # random seed for python\n",
    "# rn.seed(1254)\n",
    "# # random seed for tensorflow\n",
    "# tf.random.set_seed(89)\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "#Force Tensorflow to use a single thread\n",
    "# sess = tf.compat.v1.Session(graph=tf.compat.v1.reset_default_graph(), config=session_conf)\n",
    "# sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "# K.set_session(sess)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "# from itertools import chain\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import keras \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from collections import Counter\n",
    "# import matplotlib.pyplot as plt\n",
    "from keras.callbacks import History\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import CubicSpline      # for warping\n",
    "from transforms3d.axangles import axangle2mat  # for rotation\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "74f65436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a path like \"../trial/S01/ACC_01.parquet\" return:\n",
    "# the number of ms that a file needs in order to do the downsampling;\n",
    "# and return the type among:\n",
    "# - 0 (EDA)\n",
    "# - 1 (BVP)\n",
    "# - 2 (ACC)\n",
    "# - 3 (TEM) \n",
    "\n",
    "# If the file is incorrect return -1\n",
    "def type_file(name):\n",
    "    name_file = name.split('/')[3][:3]\n",
    "    if name_file == 'EDA':\n",
    "        return 0, 0\n",
    "    elif name_file == 'BVP':\n",
    "        return '250ms', 1\n",
    "    elif name_file == 'ACC':\n",
    "        return '250ms', 2\n",
    "    elif name_file == 'TEM':\n",
    "        return 0, 5\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aaa7661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the most frequent element in a list:\n",
    "# [0,0,0,1,1] return 0\n",
    "# [0,0,1,1,1] return 1\n",
    "\n",
    "def most_frequent(List):\n",
    "    counter = 0\n",
    "    num = List[0]\n",
    "\n",
    "    for i in List:\n",
    "        curr_frequency = List.count(i)\n",
    "        if(curr_frequency> counter):\n",
    "            counter = curr_frequency\n",
    "            num = i\n",
    "    return num\n",
    "\n",
    "# listt = [0,1,0,1]\n",
    "# print(most_frequent(listt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1f6fe9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data_and_fill_missing_nan(ARRAY, df, value):\n",
    "    for i in range(0, df.shape[0]):\n",
    "        ARRAY.append(df[value][i])\n",
    "\n",
    "    if (len(ARRAY) % 2400) != 0:\n",
    "        rounding = int(len(ARRAY)//2400 + (len(ARRAY) % 2400 > 0))\n",
    "        number_cells = rounding * 2400\n",
    "        missing_zeros = [np.nan] * (number_cells - len(ARRAY))\n",
    "        ARRAY = ARRAY + missing_zeros\n",
    "    return ARRAY\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c231d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sleep_label(LABEL, EDA, df):\n",
    "    i = 0\n",
    "    list_sleep = []\n",
    "#     create the list_sleep with all the values of the colums 'Sleep'\n",
    "    for j in range(0, df.shape[0]):\n",
    "        value = df['Sleep'][j].astype(np.float64)\n",
    "        list_sleep.append(value)\n",
    "\n",
    "#   for each 2400 window choose if 0 or 1, and append it to ARRAR_LABEL\n",
    "    while i in range(0, len(list_sleep)- 2399):\n",
    "        list_sleep_window = list_sleep[i:(i+2400)]\n",
    "        label_sleep = most_frequent(list_sleep_window)\n",
    "        LABEL.append(label_sleep)\n",
    "        i += 2400\n",
    "        \n",
    "#     if EDA len is greather than ARRAY_LABEL, uniform it to the same length\n",
    "    if (len(LABEL) < int(len(EDA)/2400)):\n",
    "        missing_zeros = [np.nan]*(int(len(EDA)/2400)-len(LABEL))\n",
    "        LABEL = LABEL + missing_zeros\n",
    "    return LABEL\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9e1b4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zeros_to_uniform_size_array(ARRAY, missing_zeros):\n",
    "    ARRAY = [np.nan]* missing_zeros\n",
    "    return ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f138719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_arrays(ARRAY, ARR2, ARR3, ARR4, ARR5, ARR6, LABEL):\n",
    "    array_zeros = np.zeros(2400)\n",
    "    \n",
    "    array_boolean = np.isnan(ARRAY).any(axis=1)\n",
    "\n",
    "    # iterate over each row.\n",
    "    # if delete a \"true\" then continue in a while untill it goes 1 step forward\n",
    "    i = 0\n",
    "    while i < len(array_boolean):\n",
    "        while array_boolean[i]:\n",
    "            ARRAY = np.delete(ARRAY, i, 0)\n",
    "            ARR2 = np.delete(ARR2, i, 0)\n",
    "            ARR3 = np.delete(ARR3, i, 0)\n",
    "            ARR4 = np.delete(ARR4, i, 0)\n",
    "            ARR5 = np.delete(ARR5, i, 0)\n",
    "            ARR6 = np.delete(ARR6, i, 0)\n",
    "            LABEL = np.delete(LABEL, i, 0)\n",
    "            array_boolean = np.delete(array_boolean, i)\n",
    "        i += 1\n",
    "    \n",
    "    ARRAY = ARRAY[:-1]\n",
    "    ARR2 = ARR2[:-1]\n",
    "    ARR3 = ARR3[:-1]\n",
    "    ARR4 = ARR4[:-1]\n",
    "    ARR5 = ARR5[:-1]\n",
    "    ARR6 = ARR6[:-1]\n",
    "    LABEL = LABEL[:-1]\n",
    "    return ARRAY, ARR2, ARR3, ARR4, ARR5, ARR6, LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bc53c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from array 1D, i create an array 2D with the last row as 0.\n",
    "def reshape_arrays(EDA):\n",
    "    EDA = np.array(EDA)\n",
    "#     print(\"eda before: \",EDA.shape)\n",
    "    EDA = EDA.reshape(1,len(EDA)//2400, 2400)\n",
    "    array_zeros = np.zeros(2400)\n",
    "    EDA = EDA[0]\n",
    "    EDA = np.concatenate((EDA, [array_zeros]), axis=0)\n",
    "    \n",
    "#     print(\"eda after: \",EDA.shape)\n",
    "    return EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d1e6ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from array 1D, i append a 0 in the last row.\n",
    "def reshape_label(LABEL):\n",
    "    LABEL = np.array(LABEL)\n",
    "#     print(\"LABEL before: \", LABEL.shape)\n",
    "#     LABEL = LABEL.reshape(1,len(LABEL), 1)\n",
    "#     array_zeros = np.zeros(2400)\n",
    "#     LABEL = LABEL[0]\n",
    "    LABEL = np.concatenate((LABEL, [0]), axis=0)\n",
    "    \n",
    "#     print(\"LABEL after: \",LABEL.shape)\n",
    "    return LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a63171cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call function create_train_set_train_label so that we have the two zero tensors.\n",
    "# for each file read the data, downsampling the data and fill the tensors.\n",
    "\n",
    "def read_data(files):\n",
    "    EDA, BVP, ACC_X, ACC_Y, ACC_Z, TEM, LABEL = [],[],[],[],[],[],[]\n",
    "\n",
    "\n",
    "    # READ ALL THE FILES and DOWNSAMPLING\n",
    "    for name_file in files:\n",
    "#         print(name_file)\n",
    "        rounding, number_cells, missing_zeros = 0, 0, []\n",
    "        # DataFrame\n",
    "        table = pd.read_parquet(name_file, engine='pyarrow')\n",
    "        # creating DataFrame\n",
    "        df = pd.DataFrame(table)\n",
    "        # converting timestamp\n",
    "        timestamp_col = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        df['timestamp'] = timestamp_col\n",
    "        # get if file it's among EDA, BVP, ACC, ST, otherwise return error\n",
    "        arr_type = type_file(name_file)\n",
    "        if arr_type == -1:\n",
    "            print(\"Error in the file name\")\n",
    "            return -1\n",
    "\n",
    "        # RESAMPLE the tensor\n",
    "        # 0 means it's already 4Hz, otherwise resample with '250ms'\n",
    "        if arr_type[0] != 0:\n",
    "            df = df.resample(arr_type[0], on='timestamp').mean()\n",
    "            df = df.reset_index()\n",
    "        \n",
    "        # remove the          \n",
    "        \n",
    "#         print(df.shape)\n",
    "       # CREATES THE ARRAYS\n",
    "        # create linear arrays as sequences of elements. for each file add several zeros to complete a window of 2400     \n",
    "        if arr_type[1] == 0:\n",
    "            EDA = append_data_and_fill_missing_nan(EDA, df, 'value')\n",
    "            LABEL = find_sleep_label(LABEL, EDA, df)\n",
    "                \n",
    "        elif arr_type[1] == 1:\n",
    "            BVP = append_data_and_fill_missing_nan(BVP, df, 'value')\n",
    "\n",
    "        elif arr_type[1] == 2:\n",
    "            ACC_X = append_data_and_fill_missing_nan(ACC_X, df, 'X')\n",
    "            ACC_Y = append_data_and_fill_missing_nan(ACC_Y, df, 'Y')\n",
    "            ACC_Z = append_data_and_fill_missing_nan(ACC_Z, df, 'Z')\n",
    "            \n",
    "        elif arr_type[1] == 5:\n",
    "            TEM = append_data_and_fill_missing_nan(TEM, df, 'value')\n",
    "        \n",
    "        \n",
    "#     print(\"LEN LABEL\",len(LABEL))\n",
    "#     if each file has the same length it doesn make sense to check it. only in between files fill the row of zeros\n",
    "#     and then start from the next row a new file\n",
    "#     if the files are not the same quantity, it's better to throw error?\n",
    "    uniform_len = max(len(EDA),len(BVP),len(ACC_X),len(ACC_Y),len(ACC_Z),len(TEM))    \n",
    "#     print(len(EDA), ' ',len(BVP), ' ',len(ACC_X), ' ',len(ACC_Y), ' ',len(ACC_Z), ' ',len(TEM) )\n",
    "#     check train_set\n",
    "    if len(EDA) != uniform_len:\n",
    "        EDA = EDA + [np.nan]* (uniform_len - len(EDA))\n",
    "    if len(BVP) != uniform_len:\n",
    "        BVP = BVP + [np.nan]* (uniform_len - len(BVP))\n",
    "    if len(ACC_X) != uniform_len:\n",
    "        ACC_X = ACC_X + [np.nan]* (uniform_len - len(ACC_X))\n",
    "    if len(ACC_Y) != uniform_len:\n",
    "        ACC_Y = ACC_Y + [np.nan]* (uniform_len - len(ACC_Y))\n",
    "    if len(ACC_Z) != uniform_len:\n",
    "        ACC_Z = ACC_Z + [np.nan]* (uniform_len - len(ACC_Z))\n",
    "    if len(TEM) != uniform_len:\n",
    "        TEM = TEM + [np.nan]* (uniform_len - len(TEM))\n",
    "        \n",
    "#     print(len(EDA_LABEL), ' ',len(BVP_LABEL), ' ',len(ACC_X_LABEL), ' ',len(ACC_Y_LABEL), ' ',len(ACC_Z_LABEL), ' ',len(TEM_LABEL) )\n",
    "    \n",
    "    \n",
    "    # convert the list to a numpy array\n",
    "#     removing rows with nan\n",
    "    EDA_np, BVP_np, TEM_np = np.array(EDA),np.array(BVP),np.array(TEM)\n",
    "    ACC_X_np, ACC_Y_np, ACC_Z_np = np.array(ACC_X),np.array(ACC_Y),np.array(ACC_Z)\n",
    "    LABEL_np = np.array(LABEL)\n",
    "\n",
    "    \n",
    "    EDA_np = reshape_arrays(EDA)\n",
    "    BVP_np = reshape_arrays(BVP)\n",
    "    ACC_X_np = reshape_arrays(ACC_X)\n",
    "    ACC_Y_np = reshape_arrays(ACC_Y)\n",
    "    ACC_Z_np = reshape_arrays(ACC_Z)\n",
    "    TEM_np = reshape_arrays(TEM)\n",
    "    LABEL_np =  reshape_label(LABEL)\n",
    "\n",
    "    \n",
    "\n",
    "    EDA_np, BVP_np, ACC_X_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np = remove_nan_arrays(EDA_np, BVP_np, ACC_X_np, ACC_Y_np, ACC_Z_np, TEM_np, LABEL_np)\n",
    "# each file has different nan rows\n",
    "    BVP_np, EDA_np, ACC_X_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np = remove_nan_arrays(BVP_np, EDA_np, ACC_X_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np)\n",
    "    ACC_X_np, EDA_np, BVP_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np = remove_nan_arrays(ACC_X_np, EDA_np, BVP_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np)\n",
    "    ACC_Y_np, EDA_np, BVP_np, ACC_X_np, ACC_Z_np, TEM_np,LABEL_np = remove_nan_arrays(ACC_Y_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, TEM_np,LABEL_np)\n",
    "    ACC_Z_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, TEM_np,LABEL_np = remove_nan_arrays(ACC_Z_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, TEM_np,LABEL_np)\n",
    "    TEM_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, ACC_Z_np,LABEL_np = remove_nan_arrays(TEM_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, ACC_Z_np,LABEL_np)\n",
    "\n",
    "\n",
    "\n",
    "    EDA = EDA_np.flatten().tolist()\n",
    "    BVP = BVP_np.flatten().tolist()\n",
    "    ACC_X = ACC_X_np.flatten().tolist()\n",
    "    ACC_Y = ACC_Y_np.flatten().tolist()\n",
    "    ACC_Z = ACC_Z_np.flatten().tolist()\n",
    "    TEM = TEM_np.flatten().tolist()\n",
    "    \n",
    "    \n",
    "    train_set = np.array([EDA, BVP, ACC_X, ACC_Y, ACC_Z, TEM],dtype=np.float64)\n",
    "    train_label = np.array([LABEL_np], dtype=np.float64)\n",
    "    \n",
    "\n",
    "    # Reshape the tensor train set, counting the number of rows dividing by 2400\n",
    "    uniform_len = max(len(EDA),len(BVP),len(ACC_X),len(ACC_Y),len(ACC_Z),len(TEM))    \n",
    "    row_training_set = uniform_len // 2400\n",
    "    if (uniform_len % 2400) != 0:\n",
    "        return \"Error in missing_zeros\"\n",
    "    \n",
    "    train_set = train_set.reshape(6,row_training_set,2400)\n",
    "    \n",
    "    print(\"before augmentation\")\n",
    "    print(train_set.shape)\n",
    "    print(train_label.shape)\n",
    "    \n",
    "\n",
    "    return train_set, train_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "be1de258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scale(X, sigma=0.1):\n",
    "    scalingFactor = np.random.normal(loc=1.0, scale=sigma, size=(1, X.shape[1]))  # shape=(1,3)\n",
    "    myNoise = np.matmul(np.ones((X.shape[0], 1)), scalingFactor)\n",
    "    return X * myNoise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "297b9b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigma = 0.05\n",
    "# Hyperparameters :  sigma = standard devitation (STD) of the noise\n",
    "def jitter(X, sigma=0.05):\n",
    "    myNoise = np.random.normal(loc=0, scale=sigma, size=X.shape)\n",
    "    print(\"jitter\")\n",
    "    return X + myNoise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e65ad2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateRandomCurves(X, sigma=0.2, knot=4):\n",
    "    xx = (np.ones((X.shape[1], 1)) * (np.arange(0, X.shape[0], (X.shape[0] - 1) / (knot + 1)))).transpose()\n",
    "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot + 2, X.shape[1]))\n",
    "    x_range = np.arange(X.shape[0])\n",
    "    random_curves = []\n",
    "    for i in range(X.shape[-1]):\n",
    "        cs = CubicSpline(xx[:, i], yy[:, i])\n",
    "        random_curves.append(cs(x_range))\n",
    "    return np.array(random_curves).transpose()\n",
    "\n",
    "def DA_MagWarp(X, sigma=0.2):\n",
    "    return X * GenerateRandomCurves(X, sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ba979397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(start_session, end_session):\n",
    "#     create list of folders\n",
    "    listt = os.listdir(\"../Sessions/\")\n",
    "#     print(listt)\n",
    "#   how many users? for now only 1\n",
    "    list_dir = sorted(listt)[0:1]\n",
    "#     list_dir = sorted(listt)\n",
    "    list_path_dir = []\n",
    "    for elem in list_dir:\n",
    "        list_path_dir.append(\"../Sessions/\" + elem)\n",
    "\n",
    "#     read files in the folder and create a list of files \n",
    "    list_ordered_files = []\n",
    "    \n",
    "    for folder in list_path_dir:\n",
    "#         print(folder)\n",
    "        list_files = []\n",
    "        for infile in os.listdir(folder):\n",
    "            list_files.append(infile)\n",
    "        list_files = sorted(list_files)\n",
    "#         print(list_files)\n",
    "\n",
    "    #     divide the list into 4 sections: acc, bvp, eda, temp\n",
    "    #     take one from each and construct a list \n",
    "    #     the order is really important while reading files\n",
    "        \n",
    "        acc = []\n",
    "        bvp = []\n",
    "        eda = []\n",
    "        tem = []\n",
    "        for elem in (list_files):\n",
    "            if elem[:3] == \"ACC\":\n",
    "                acc.append(elem)\n",
    "            elif elem[:3] == \"BVP\":\n",
    "                bvp.append(elem)\n",
    "            elif elem[:3] == \"EDA\":\n",
    "                eda.append(elem)\n",
    "            elif elem[:3] == \"TEM\":\n",
    "                tem.append(elem)\n",
    "\n",
    "        acc = acc[start_session:end_session]\n",
    "        bvp = bvp[start_session:end_session]\n",
    "        eda = eda[start_session:end_session]\n",
    "        tem = tem[start_session:end_session]\n",
    "    #     create a list with that order: [acc1, bvp1, eda1, tem1, acc2, bvp2, eda2, tem2,...]\n",
    "        for i in range(len(acc)):\n",
    "            list_ordered_files.append(folder + \"/\" + acc[i])\n",
    "            list_ordered_files.append(folder + \"/\" + bvp[i])\n",
    "            list_ordered_files.append(folder + \"/\" + eda[i])\n",
    "            list_ordered_files.append(folder + \"/\" + tem[i])\n",
    "#     print(list_ordered_files)\n",
    "        \n",
    "    \n",
    "    return read_data(list_ordered_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "34e34be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def augment_data(train_set, train_label):\n",
    "    print(train_set.shape)\n",
    "#     i have to compat the ACC_X, ACC_Y, ACC_Z to ACC with formula √(x*x + y*y + z*z)\n",
    "    acc_x_squared = np.multiply(train_set[2], train_set[2])\n",
    "    acc_y_squared = np.multiply(train_set[3], train_set[3])\n",
    "    acc_z_squared = np.multiply(train_set[4], train_set[4])\n",
    "    acc_xyz_sum = acc_x_squared + acc_y_squared + acc_z_squared\n",
    "    acc = np.sqrt(acc_xyz_sum)\n",
    "    \n",
    "#     train_set = np.array([train_set[0], train_set[1], acc, train_set[5]])\n",
    "    print(train_set.shape)\n",
    "    \n",
    "#     START AUGMENTING\n",
    "    ARR,LABEL = [], []\n",
    "    \n",
    "#     print(train_set.shape)\n",
    "#     print(train_label.shape)\n",
    "    # select random indices\n",
    "    number_of_rows = int(train_set.shape[1] * 0.5)\n",
    "\n",
    "#     random indices has to be the same for every dimension so that the label can be accurate\n",
    "    random_indices = np.sort(np.random.choice(train_set.shape[1]-1, size=int(number_of_rows), replace=False))\n",
    "    #     iterate over every dimension\n",
    "#     augment every dimension of the tensor so that at the end we have a tensor augmented\n",
    "    for train_set_one in train_set:\n",
    "    # take partial array with random indices\n",
    "        train_set_one = train_set_one[random_indices,:]\n",
    "    # perform jittering on the partial array\n",
    "        train_set_one = train_set_one.transpose()\n",
    "#         SCALE AUGMENT\n",
    "        train_set_one = scale(train_set_one)\n",
    "        train_set_one = train_set_one.transpose()\n",
    "        \n",
    "#     create an array ARR only of augmented data\n",
    "        ARR = [*ARR, train_set_one]\n",
    "\n",
    "    ARR = np.array(ARR)\n",
    "    # take the label and add them as the label for the new augmented data\n",
    "    LABEL = np.array(train_label[:,random_indices])\n",
    "\n",
    "#     we have ARR which is of shape (6, row, col) with the augmented data\n",
    "#     and train_set which is of shape (6, row, col) with the non augmented data\n",
    "    train_set_augmented = np.concatenate((train_set, ARR), axis = 1)\n",
    "    \n",
    "    label_set_augmented = np.concatenate((train_label, LABEL), axis=1)\n",
    "#     print(label_set_augmented.shape)\n",
    "#     print(train_set_augmented.shape)\n",
    "    \n",
    "    return train_set_augmented,label_set_augmented\n",
    "\n",
    "# augment_data(train_set, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "35ef772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmented_data(train_set, train_label):\n",
    "    print(\"start augmenting\")\n",
    "    train_set_augmented, label_set_augmented = augment_data(train_set, train_label)\n",
    "#     reshape the label for the networking part\n",
    "    train_label = train_label.reshape(train_label.shape[1], 1)\n",
    "    label_set_augmented = label_set_augmented.reshape(label_set_augmented.shape[1], 1)\n",
    "    \n",
    "    print(train_set.shape)\n",
    "    print(train_label.shape)\n",
    "    print(train_set_augmented.shape)\n",
    "    print(label_set_augmented.shape)\n",
    "    print(\"end augmenting\")\n",
    "    \n",
    "    return train_set, train_label, train_set_augmented, label_set_augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3fcf96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4 is the number of sessions per user\n",
    "# train_set, train_label = read_files(0,4)\n",
    "# from 0 session to 5 session per user\n",
    "train_set, train_label =  create_datasets(0,4)\n",
    "train_set, train_label, train_set_augmented, label_set_augmented = get_augmented_data(train_set, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fd2058f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before augmentation\n",
      "(6, 721, 2400)\n",
      "(1, 721)\n"
     ]
    }
   ],
   "source": [
    "# create the test set\n",
    "test_set, test_label = create_datasets(5,6)\n",
    "test_label = test_label.reshape(test_label.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compat_accelerometer(train_set, train_label):\n",
    "    train_acc = \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9de5dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "33552959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2689, 2400)\n",
      "(721, 2400)\n",
      "Accuracy of 50 fit of original set:  0.5312066674232483\n",
      "Accuracy of 50 fit of EDA set:  0.6352288722991943\n",
      "-----END-----\n"
     ]
    }
   ],
   "source": [
    "seed_value= 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# rn.seed(seed_value)\n",
    "# np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "\n",
    "\n",
    "def network_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(2400,)))\n",
    "#     model.add(Dense(16, activation='relu'))\n",
    "#     model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    \n",
    "#     ORIGINAL SET\n",
    "    train_set_eda = train_set[0]\n",
    "    test_set_eda = test_set[0]\n",
    "    print(train_set_eda.shape)\n",
    "#     print(test_label.shape)\n",
    "    print(test_set_eda.shape)\n",
    "    \n",
    "    avg_accuracy = 0\n",
    "    for i in range(0, 1):\n",
    "        histor = model.fit(train_set_eda, train_label, epochs=15, batch_size=128,shuffle=False, verbose=0)\n",
    "        scores = model.evaluate(test_set_eda, test_label,verbose=0)\n",
    "        avg_accuracy += scores[1]\n",
    "#         print('Test loss: {} - Accuracy: {}'.format(*scores))\n",
    "#         avg = reduce(lambda x, y: x + y, histor.history['accuracy']) / len(histor.history['accuracy'])\n",
    "#         print(avg)\n",
    "    print(\"Accuracy of 50 fit of original set: \", avg_accuracy/1)\n",
    "    \n",
    "#     AUGMENTATION\n",
    "    train_set_eda_augment = train_set_augmented[0]\n",
    "#     print(train_set_eda_augment.shape)\n",
    "#     print(label_set_augmented.shape)\n",
    "\n",
    "    avg_accuracy = 0\n",
    "    for i in range(0, 1):\n",
    "        histor = model.fit(train_set_eda_augment, label_set_augmented, epochs=15, batch_size=128,shuffle=False,verbose=0)\n",
    "        scores = model.evaluate(test_set_eda, test_label,verbose=0)\n",
    "        avg_accuracy += scores[1]\n",
    "#         print('Test loss: {} - Accuracy: {}'.format(*scores))\n",
    "#         avg = reduce(lambda x, y: x + y, histor.history['accuracy']) / len(histor.history['accuracy'])\n",
    "#         print(avg)\n",
    "    print(\"Accuracy of 50 fit of EDA set: \", avg_accuracy/1)\n",
    "    print(\"-----END-----\")\n",
    "\n",
    "network_model()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6b5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3819ff39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fda1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a595f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c39a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0ecd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ad341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e41fbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae35859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC BVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b319f192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2689, 2400)\n",
      "(721, 2400)\n",
      "Accuracy of 50 fit of original set:  0.484049916267395\n",
      "Accuracy of 50 fit of EDA set:  0.4882108271121979\n",
      "-----END-----\n"
     ]
    }
   ],
   "source": [
    "seed_value= 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "# rn.seed(seed_value)\n",
    "# np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "\n",
    "def network_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(2400,)))\n",
    "#     model.add(Dense(16, activation='relu'))\n",
    "#     model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    \n",
    "#     ORIGINAL SET\n",
    "    train_set_eda = train_set[1]\n",
    "    test_set_eda = test_set[1]\n",
    "    print(train_set_eda.shape)\n",
    "#     print(test_label.shape)\n",
    "    print(test_set_eda.shape)\n",
    "    \n",
    "    avg_accuracy = 0\n",
    "    for i in range(0, 1):\n",
    "        histor = model.fit(train_set_eda, train_label, epochs=15, batch_size=128,shuffle=False, verbose=0)\n",
    "        scores = model.evaluate(test_set_eda, test_label,verbose=0)\n",
    "        avg_accuracy += scores[1]\n",
    "#         print('Test loss: {} - Accuracy: {}'.format(*scores))\n",
    "#         avg = reduce(lambda x, y: x + y, histor.history['accuracy']) / len(histor.history['accuracy'])\n",
    "#         print(avg)\n",
    "    print(\"Accuracy of 50 fit of original set: \", avg_accuracy/1)\n",
    "    \n",
    "#     AUGMENTATION\n",
    "    train_set_eda_augment = train_set_augmented[1]\n",
    "#     print(train_set_eda_augment.shape)\n",
    "#     print(label_set_augmented.shape)\n",
    "\n",
    "    avg_accuracy = 0\n",
    "    for i in range(0, 1):\n",
    "        histor = model.fit(train_set_eda_augment, label_set_augmented, epochs=15, batch_size=128,shuffle=False,verbose=0)\n",
    "        scores = model.evaluate(test_set_eda, test_label,verbose=0)\n",
    "        avg_accuracy += scores[1]\n",
    "#         print('Test loss: {} - Accuracy: {}'.format(*scores))\n",
    "#         avg = reduce(lambda x, y: x + y, histor.history['accuracy']) / len(histor.history['accuracy'])\n",
    "#         print(avg)\n",
    "    print(\"Accuracy of 50 fit of EDA set: \", avg_accuracy/1)\n",
    "    print(\"-----END-----\")\n",
    "\n",
    "network_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed63e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373551a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847deea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da3a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba0b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0caa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2f4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c55f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5141410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626853b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18a840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02098c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b54b426e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2689, 2400)\n",
      "(721, 2400)\n",
      "Accuracy of 50 fit of original set:  0.5284327268600464\n",
      "Accuracy of 50 fit of EDA set:  0.47295424342155457\n",
      "-----END-----\n"
     ]
    }
   ],
   "source": [
    "seed_value= 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "rn.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "\n",
    "def network_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(2400,)))\n",
    "#     model.add(Dense(16, activation='relu'))\n",
    "#     model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    \n",
    "#     ORIGINAL SET\n",
    "    train_set_eda = train_set[5]\n",
    "    test_set_eda = test_set[5]\n",
    "    print(train_set_eda.shape)\n",
    "#     print(test_label.shape)\n",
    "    print(test_set_eda.shape)\n",
    "    \n",
    "    avg_accuracy = 0\n",
    "    for i in range(0, 1):\n",
    "        histor = model.fit(train_set_eda, train_label, epochs=15, batch_size=128,shuffle=False, verbose=0)\n",
    "        scores = model.evaluate(test_set_eda, test_label,verbose=0)\n",
    "        avg_accuracy += scores[1]\n",
    "#         print('Test loss: {} - Accuracy: {}'.format(*scores))\n",
    "#         avg = reduce(lambda x, y: x + y, histor.history['accuracy']) / len(histor.history['accuracy'])\n",
    "#         print(avg)\n",
    "    print(\"Accuracy of 50 fit of original set: \", avg_accuracy/1)\n",
    "    \n",
    "#     AUGMENTATION\n",
    "    train_set_eda_augment = train_set_augmented[5]\n",
    "#     print(train_set_eda_augment.shape)\n",
    "#     print(label_set_augmented.shape)\n",
    "\n",
    "    avg_accuracy = 0\n",
    "    for i in range(0, 1):\n",
    "        histor = model.fit(train_set_eda_augment, label_set_augmented, epochs=15, batch_size=128,shuffle=False,verbose=0)\n",
    "        scores = model.evaluate(test_set_eda, test_label,verbose=0)\n",
    "        avg_accuracy += scores[1]\n",
    "#         print('Test loss: {} - Accuracy: {}'.format(*scores))\n",
    "#         avg = reduce(lambda x, y: x + y, histor.history['accuracy']) / len(histor.history['accuracy'])\n",
    "#         print(avg)\n",
    "    print(\"Accuracy of 50 fit of EDA set: \", avg_accuracy/1)\n",
    "    print(\"-----END-----\")\n",
    "\n",
    "network_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b8bb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9d5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce175d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017f16e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7920272a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8f734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c2749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d2db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4f0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b829e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49b2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a18ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa14c7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22936a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7e6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f95e3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3600 samples (Time) * 3 features (X,Y,Z)\n",
    "# take two dimensions array\n",
    "\n",
    "# [[acc_x_0], [acc_y_0], [acc_z_0]]\n",
    "eda = train_set[0][0]\n",
    "bvp = train_set[1][0]\n",
    "acc_x_0 = train_set[2][0]\n",
    "acc_y_0 = train_set[3][0]\n",
    "acc_z_0 = train_set[4][0]\n",
    "tem = train_set[5][0]\n",
    "\n",
    "\n",
    "array_to_plot = np.array([acc_x_0,acc_y_0,acc_z_0], dtype=np.float64)\n",
    "# plt.plot(array_to_plot)\n",
    "\n",
    "# plt.plot(acc_x_0, color='g')\n",
    "# plt.plot(acc_y_0, color='r')\n",
    "# plt.plot(acc_z_0, color='b')\n",
    "# plt.title(\"A window of 10 min Acceleration data\")\n",
    "# plt.axis([0,2400,-100,100])\n",
    "# plt.xlabel(\"time\")\n",
    "# plt.ylabel(\"value acc\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "plt.plot(eda, color='g')\n",
    "plt.title(\"A window of 10 min Electrodermal Activity data\")\n",
    "plt.axis([0,2400,-0.5,1.5])\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"value acc\")\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(bvp, color='g')\n",
    "# plt.title(\"A window of 10 min Blood Volume Pulse data\")\n",
    "# plt.axis([0,2400,-450, 450])\n",
    "# plt.xlabel(\"time\")\n",
    "# plt.ylabel(\"value acc\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(tem, color='g')\n",
    "# plt.title(\"A window of 10 min Skin Temperature data\")\n",
    "# plt.axis([0,2400,28, 29.5])\n",
    "# plt.xlabel(\"time\")\n",
    "# plt.ylabel(\"value acc\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sigma = 0.05\n",
    "\n",
    "def DA_Jitter(X, sigma=0.05):\n",
    "    myNoise = np.random.normal(loc=0, scale=sigma, size=X.shape)\n",
    "    return X+myNoise\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,4))\n",
    "for ii in range(8):\n",
    "    ax = fig.add_subplot(2,4,ii+1)\n",
    "    ax.plot(DA_Jitter(eda, sigma))\n",
    "    ax.set_xlim([0,2400])\n",
    "    ax.set_ylim([-0.5,1.5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "049805b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 1 1]\n",
      "  [2 2 2]]\n",
      "\n",
      " [[3 3 3]\n",
      "  [4 4 4]]]\n",
      "\n",
      "[[[5 5 5]\n",
      "  [6 6 6]]\n",
      "\n",
      " [[7 7 7]\n",
      "  [0 0 0]]]\n",
      "\n",
      "[[[ 6  6  6]\n",
      "  [ 8  8  8]]\n",
      "\n",
      " [[10 10 10]\n",
      "  [ 4  4  4]]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[[1,1,1], [2,2,2]], [[3,3,3],[4,4,4]]])\n",
    "b = np.array([[[5,5,5], [6,6,6]], [[7,7,7],[0,0,0]]])\n",
    "print(a)\n",
    "print()\n",
    "print(b)\n",
    "print()\n",
    "# print(np.multiply(a, b))\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c15280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
