{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "from itertools import chain\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import os\n",
    "import keras \n",
    "import sys\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import History\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a path like \"../trial/S01/ACC_01.parquet\" return:\n",
    "# the number of ms that a file needs in order to do the downsampling;\n",
    "# and return the type among:\n",
    "# - 0 (EDA)\n",
    "# - 1 (BVP)\n",
    "# - 2 (ACC)\n",
    "# - 3 (TEM) \n",
    "\n",
    "# If the file is incorrect return -1\n",
    "def type_file(name):\n",
    "    name_file = name.split('/')[3][:3]\n",
    "#     print(name_file)\n",
    "    if name_file == 'EDA':\n",
    "        return 0, 0\n",
    "    elif name_file == 'BVP':\n",
    "        return '250ms', 1\n",
    "    elif name_file == 'ACC':\n",
    "        return '250ms', 2\n",
    "    elif name_file == 'TEM':\n",
    "        return 0, 5\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the most frequent element in a list:\n",
    "# [0,0,0,1,1] return 0\n",
    "# [0,0,1,1,1] return 1\n",
    "\n",
    "def most_frequent(List):\n",
    "    counter = 0\n",
    "    num = List[0]\n",
    "\n",
    "    for i in List:\n",
    "        curr_frequency = List.count(i)\n",
    "        if(curr_frequency> counter):\n",
    "            counter = curr_frequency\n",
    "            num = i\n",
    "    return num\n",
    "\n",
    "\n",
    "# listt = [0,1,0,1]\n",
    "# print(most_frequent(listt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data_and_fill_missing_zeros(ARRAY, df, value):\n",
    "    for i in range(0, df.shape[0]):\n",
    "        ARRAY.append(df[value][i])\n",
    "#         print(\"BEFORE: \", len(ARRAY))\n",
    "\n",
    "    if (len(ARRAY) % 2400) != 0:\n",
    "        rounding = int(len(ARRAY)//2400 + (len(ARRAY) % 2400 > 0))\n",
    "        number_cells = rounding * 2400\n",
    "        missing_zeros = [0] * (number_cells - len(ARRAY))\n",
    "        ARRAY = ARRAY + missing_zeros\n",
    "    return ARRAY\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sleep_label(LABEL, EDA, df):\n",
    "    i = 0\n",
    "    list_sleep = []\n",
    "#     create the list_sleep with all the values of the colums 'Sleep'\n",
    "    for j in range(0, df.shape[0]):\n",
    "        value = df['Sleep'][j].astype(np.float64)\n",
    "        list_sleep.append(value)\n",
    "\n",
    "#   for each 2400 window choose if 0 or 1, and append it to ARRAR_LABEL\n",
    "    while i in range(0, len(list_sleep)- 2399):\n",
    "        list_sleep_window = list_sleep[i:(i+2400)]\n",
    "        label_sleep = most_frequent(list_sleep_window)\n",
    "        LABEL.append(label_sleep)\n",
    "        i += 2400\n",
    "        \n",
    "#     if EDA len is greather than ARRAY_LABEL, uniform it to the same length\n",
    "    if (len(LABEL) < int(len(EDA)/2400)):\n",
    "        missing_zeros = [0]*(int(len(EDA)/2400)-len(LABEL))\n",
    "        LABEL = LABEL + missing_zeros\n",
    "        \n",
    "#     print(len(LABEL))\n",
    "    return LABEL\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zeros_to_uniform_size_array(ARRAY, missing_zeros):\n",
    "    ARRAY = [0]* missing_zeros\n",
    "    return ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call function create_train_set_train_label so that we have the two zero tensors.\n",
    "# for each file read the data, downsampling the data and fill the tensors.\n",
    "\n",
    "def read_data(files):\n",
    "    EDA, BVP, ACC_X, ACC_Y, ACC_Z, TEM, LABEL = [],[],[],[],[],[],[]\n",
    "\n",
    "    \n",
    "    # READ ALL THE FILES and DOWNSAMPLING\n",
    "    for name_file in files:\n",
    "#         print(name_file)\n",
    "        rounding, number_cells, missing_zeros = 0, 0, []\n",
    "        # DataFrame\n",
    "        table = pd.read_parquet(name_file, engine='pyarrow')\n",
    "        # creating DataFrame\n",
    "        df = pd.DataFrame(table)\n",
    "        # converting timestamp\n",
    "        timestamp_col = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        df['timestamp'] = timestamp_col\n",
    "        # get if file it's among EDA, BVP, ACC, ST, otherwise return error\n",
    "        arr_type = type_file(name_file)\n",
    "        if arr_type == -1:\n",
    "            print(\"Error in the file name\")\n",
    "            return -1\n",
    "\n",
    "        # RESAMPLE the tensor\n",
    "        # 0 means it's already 4Hz, otherwise resample with '250ms'\n",
    "        if arr_type[0] != 0:\n",
    "            df = df.resample(arr_type[0], on='timestamp').mean()\n",
    "            df = df.reset_index()\n",
    "        \n",
    "        \n",
    "        print(df.shape)\n",
    "       # CREATES THE ARRAYS\n",
    "        # create linear arrays as sequences of elements. for each file add several zeros to complete a window of 2400     \n",
    "        if arr_type[1] == 0:\n",
    "            EDA = append_data_and_fill_missing_zeros(EDA, df, 'value')\n",
    "            LABEL = find_sleep_label(LABEL, EDA, df)\n",
    "                \n",
    "        elif arr_type[1] == 1:\n",
    "            BVP = append_data_and_fill_missing_zeros(BVP, df, 'value')\n",
    "\n",
    "        elif arr_type[1] == 2:\n",
    "            ACC_X = append_data_and_fill_missing_zeros(ACC_X, df, 'X')\n",
    "            ACC_Y = append_data_and_fill_missing_zeros(ACC_Y, df, 'Y')\n",
    "            ACC_Z = append_data_and_fill_missing_zeros(ACC_Z, df, 'Z')\n",
    "            \n",
    "        elif arr_type[1] == 5:\n",
    "            TEM = append_data_and_fill_missing_zeros(TEM, df, 'value')\n",
    "        \n",
    "        \n",
    "#     print(\"LEN LABEL\",len(LABEL))\n",
    "#     if each file has the same length it doesn make sense to check it. only in between files fill the row of zeros\n",
    "#     and then start from the next row a new file\n",
    "#     if the files are not the same quantity, it's better to throw error?\n",
    "    uniform_len = max(len(EDA),len(BVP),len(ACC_X),len(ACC_Y),len(ACC_Z),len(TEM))    \n",
    "#     print(len(EDA), ' ',len(BVP), ' ',len(ACC_X), ' ',len(ACC_Y), ' ',len(ACC_Z), ' ',len(TEM) )\n",
    "#     check train_set\n",
    "    if len(EDA) != uniform_len:\n",
    "        EDA = EDA + [0]* (uniform_len - len(EDA))\n",
    "    if len(BVP) != uniform_len:\n",
    "        BVP = BVP + [0]* (uniform_len - len(BVP))\n",
    "    if len(ACC_X) != uniform_len:\n",
    "        ACC_X = ACC_X + [0]* (uniform_len - len(ACC_X))\n",
    "    if len(ACC_Y) != uniform_len:\n",
    "        ACC_Y = ACC_Y + [0]* (uniform_len - len(ACC_Y))\n",
    "    if len(ACC_Z) != uniform_len:\n",
    "        ACC_Z = ACC_Z + [0]* (uniform_len - len(ACC_Z))\n",
    "    if len(TEM) != uniform_len:\n",
    "        TEM = TEM + [0]* (uniform_len - len(TEM))\n",
    "        \n",
    "\n",
    "#     print(len(EDA_LABEL), ' ',len(BVP_LABEL), ' ',len(ACC_X_LABEL), ' ',len(ACC_Y_LABEL), ' ',len(ACC_Z_LABEL), ' ',len(TEM_LABEL) )\n",
    "    # convert the list to a numpy array\n",
    "    train_set = np.array([EDA, BVP, ACC_X, ACC_Y, ACC_Z, TEM],dtype=np.float64)\n",
    "    train_label = np.array([LABEL], dtype=np.float64)\n",
    "    \n",
    "\n",
    "    # Reshape the tensor train set, counting the number of rows dividing by 2400\n",
    "    row_training_set = uniform_len // 2400\n",
    "    if (uniform_len % 2400) != 0:\n",
    "        return \"Error in missing_zeros\"\n",
    "    train_set = train_set.reshape(6,row_training_set,2400)\n",
    "#     Reshape the tensor train label\n",
    "#     train_label = train_label.reshape(6,uniform_len_label, 1)\n",
    "    \n",
    "\n",
    "    train_set = np.dstack(train_set)\n",
    "    train_label = train_label.reshape(train_label.shape[1], 1)\n",
    "    print(train_set.shape)\n",
    "    print(train_label.shape)\n",
    "    \n",
    "\n",
    "    print(\"END read data\")\n",
    "    return train_set, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_model(train_set, train_label) :\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, activation='relu', input_shape=(2400, 6), name=\"layer1\"))\n",
    "    model.add(Dense(12, activation='relu', name=\"layer2\"))\n",
    "    model.add(Dense(1, activation='sigmoid', name=\"layer3\"))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(np.array(train_set), np.array(train_label), epochs=15, batch_size=128)\n",
    "    print(\"END network model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(start_session, end_session):\n",
    "#     create list of folders\n",
    "    listt = os.listdir(\"../Sessions/\")\n",
    "\n",
    "#     list_dir = sorted(listt)[8:12]\n",
    "    list_dir = sorted(listt)\n",
    "    list_path_dir = []\n",
    "    for elem in list_dir:\n",
    "        list_path_dir.append(\"../Sessions/\" + elem)\n",
    "\n",
    "#     read files in the folder and create a list of files \n",
    "    list_ordered_files = []\n",
    "    \n",
    "    for folder in list_path_dir:\n",
    "#         print(folder)\n",
    "        list_files = []\n",
    "        for infile in os.listdir(folder):\n",
    "            list_files.append(infile)\n",
    "        list_files = sorted(list_files)\n",
    "#         print(list_files)\n",
    "\n",
    "    #     divide the list into 4 sections: acc, bvp, eda, temp\n",
    "    #     take one from each and construct a list \n",
    "    #     the order is really important while reading files\n",
    "        \n",
    "        acc = []\n",
    "        bvp = []\n",
    "        eda = []\n",
    "        tem = []\n",
    "        for elem in (list_files):\n",
    "            if elem[:3] == \"ACC\":\n",
    "                acc.append(elem)\n",
    "            elif elem[:3] == \"BVP\":\n",
    "                bvp.append(elem)\n",
    "            elif elem[:3] == \"EDA\":\n",
    "                eda.append(elem)\n",
    "            elif elem[:3] == \"TEM\":\n",
    "                tem.append(elem)\n",
    "\n",
    "        acc = acc[start_session:end_session]\n",
    "        bvp = bvp[start_session:end_session]\n",
    "        eda = eda[start_session:end_session]\n",
    "        tem = tem[start_session:end_session]\n",
    "    #     create a list with that order: [acc1, bvp1, eda1, tem1, acc2, bvp2, eda2, tem2,...]\n",
    "        for i in range(len(acc)):\n",
    "            list_ordered_files.append(folder + \"/\" + acc[i])\n",
    "            list_ordered_files.append(folder + \"/\" + bvp[i])\n",
    "            list_ordered_files.append(folder + \"/\" + eda[i])\n",
    "            list_ordered_files.append(folder + \"/\" + tem[i])\n",
    "#     print(list_ordered_files)\n",
    "        \n",
    "    \n",
    "    return read_data(list_ordered_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178064, 6)\n",
      "(178064, 4)\n",
      "(178064, 4)\n",
      "(178064, 4)\n",
      "(118536, 6)\n",
      "(118536, 4)\n",
      "(118536, 4)\n",
      "(118536, 4)\n",
      "(127292, 6)\n",
      "(127292, 4)\n",
      "(127292, 4)\n",
      "(127292, 4)\n",
      "(179620, 6)\n",
      "(179620, 4)\n",
      "(179620, 4)\n",
      "(179620, 4)\n",
      "(213868, 6)\n",
      "(213868, 4)\n",
      "(213868, 4)\n",
      "(213868, 4)\n",
      "(234096, 6)\n",
      "(234096, 4)\n",
      "(234096, 4)\n",
      "(234096, 4)\n",
      "(215920, 6)\n",
      "(215920, 4)\n",
      "(215920, 4)\n",
      "(215920, 4)\n",
      "(227504, 6)\n",
      "(227504, 4)\n",
      "(227504, 4)\n",
      "(227504, 4)\n",
      "(223204, 6)\n",
      "(223204, 4)\n",
      "(223204, 4)\n",
      "(223204, 4)\n",
      "(225564, 6)\n",
      "(225564, 4)\n",
      "(225564, 4)\n",
      "(225564, 4)\n",
      "(210372, 6)\n",
      "(210372, 4)\n",
      "(210372, 4)\n",
      "(210372, 4)\n",
      "(231104, 6)\n",
      "(231104, 4)\n",
      "(231104, 4)\n",
      "(231104, 4)\n",
      "(190148, 6)\n",
      "(190148, 4)\n",
      "(190148, 4)\n",
      "(190148, 4)\n",
      "(142856, 6)\n",
      "(142856, 4)\n",
      "(142856, 4)\n",
      "(142856, 4)\n",
      "(213716, 6)\n",
      "(213716, 4)\n",
      "(213716, 4)\n",
      "(213716, 4)\n",
      "(223320, 6)\n",
      "(223320, 4)\n",
      "(223320, 4)\n",
      "(223320, 4)\n",
      "(225860, 6)\n",
      "(225860, 4)\n",
      "(225860, 4)\n",
      "(225860, 4)\n",
      "(220156, 6)\n",
      "(220156, 4)\n",
      "(220156, 4)\n",
      "(220156, 4)\n",
      "(227520, 6)\n",
      "(227520, 4)\n",
      "(227520, 4)\n",
      "(227520, 4)\n",
      "(216128, 6)\n",
      "(216128, 4)\n",
      "(216128, 4)\n",
      "(216128, 4)\n",
      "(227188, 6)\n",
      "(227188, 4)\n",
      "(227188, 4)\n",
      "(227188, 4)\n",
      "(130448, 6)\n",
      "(130448, 4)\n",
      "(130448, 4)\n",
      "(130448, 4)\n",
      "(218688, 6)\n",
      "(218688, 4)\n",
      "(218688, 4)\n",
      "(218688, 4)\n",
      "(139104, 6)\n",
      "(139104, 4)\n",
      "(139104, 4)\n",
      "(139104, 4)\n",
      "(145364, 6)\n",
      "(145364, 4)\n",
      "(145364, 4)\n",
      "(145364, 4)\n",
      "(159972, 6)\n",
      "(159972, 4)\n",
      "(159972, 4)\n",
      "(159972, 4)\n",
      "(127500, 6)\n",
      "(127500, 4)\n",
      "(127500, 4)\n",
      "(127500, 4)\n",
      "(117092, 6)\n",
      "(117092, 4)\n",
      "(117092, 4)\n",
      "(117092, 4)\n",
      "(235188, 6)\n",
      "(235188, 4)\n",
      "(235188, 4)\n",
      "(235188, 4)\n",
      "(218324, 6)\n",
      "(218324, 4)\n",
      "(218324, 4)\n",
      "(218324, 4)\n",
      "(235468, 6)\n",
      "(235468, 4)\n",
      "(235468, 4)\n",
      "(235468, 4)\n",
      "(246868, 6)\n",
      "(246868, 4)\n",
      "(246868, 4)\n",
      "(246868, 4)\n",
      "(189588, 6)\n",
      "(189588, 4)\n",
      "(189588, 4)\n",
      "(189588, 4)\n",
      "(240912, 6)\n",
      "(240912, 4)\n",
      "(240912, 4)\n",
      "(240912, 4)\n",
      "(200072, 6)\n",
      "(200072, 4)\n",
      "(200072, 4)\n",
      "(200072, 4)\n",
      "(219720, 6)\n",
      "(219720, 4)\n",
      "(219720, 4)\n",
      "(219720, 4)\n",
      "(206336, 6)\n",
      "(206336, 4)\n",
      "(206336, 4)\n",
      "(206336, 4)\n",
      "(210020, 6)\n",
      "(210020, 4)\n",
      "(210020, 4)\n",
      "(210020, 4)\n",
      "(129600, 6)\n",
      "(129600, 4)\n",
      "(129600, 4)\n",
      "(129600, 4)\n",
      "(225716, 6)\n",
      "(225716, 4)\n",
      "(225716, 4)\n",
      "(225716, 4)\n",
      "(180316, 6)\n",
      "(180316, 4)\n",
      "(180316, 4)\n",
      "(180316, 4)\n",
      "(129360, 6)\n",
      "(129360, 4)\n",
      "(129360, 4)\n",
      "(129360, 4)\n",
      "(126000, 6)\n",
      "(126000, 4)\n",
      "(126000, 4)\n",
      "(126000, 4)\n",
      "(211784, 6)\n",
      "(211784, 4)\n",
      "(211784, 4)\n",
      "(211784, 4)\n",
      "(212684, 6)\n",
      "(212684, 4)\n",
      "(212684, 4)\n",
      "(212684, 4)\n",
      "(201344, 6)\n",
      "(201344, 4)\n",
      "(201344, 4)\n",
      "(201344, 4)\n",
      "(130020, 6)\n",
      "(130020, 4)\n",
      "(130020, 4)\n",
      "(130020, 4)\n",
      "(218300, 6)\n",
      "(218300, 4)\n",
      "(218300, 4)\n",
      "(218300, 4)\n",
      "(205104, 6)\n",
      "(205104, 4)\n",
      "(205104, 4)\n",
      "(205104, 4)\n",
      "(240736, 6)\n",
      "(240736, 4)\n",
      "(240736, 4)\n",
      "(240736, 4)\n",
      "(252636, 6)\n",
      "(252636, 4)\n",
      "(252636, 4)\n",
      "(252636, 4)\n",
      "(243976, 6)\n",
      "(243976, 4)\n",
      "(243976, 4)\n",
      "(243976, 4)\n",
      "(229924, 6)\n",
      "(229924, 4)\n",
      "(229924, 4)\n",
      "(229924, 4)\n",
      "(234416, 6)\n",
      "(234416, 4)\n",
      "(234416, 4)\n",
      "(234416, 4)\n",
      "(231572, 6)\n",
      "(231572, 4)\n",
      "(231572, 4)\n",
      "(231572, 4)\n",
      "(224312, 6)\n",
      "(224312, 4)\n",
      "(224312, 4)\n",
      "(224312, 4)\n",
      "(217200, 6)\n",
      "(217200, 4)\n",
      "(217200, 4)\n",
      "(217200, 4)\n",
      "(209760, 6)\n",
      "(209760, 4)\n",
      "(209760, 4)\n",
      "(209760, 4)\n",
      "(235200, 6)\n",
      "(235200, 4)\n",
      "(235200, 4)\n",
      "(235200, 4)\n",
      "(174480, 6)\n",
      "(174480, 4)\n",
      "(174480, 4)\n",
      "(174480, 4)\n",
      "(225776, 6)\n",
      "(225776, 4)\n",
      "(225776, 4)\n",
      "(225776, 4)\n",
      "(219880, 6)\n",
      "(219880, 4)\n",
      "(219880, 4)\n",
      "(219880, 4)\n",
      "(225600, 6)\n",
      "(225600, 4)\n",
      "(225600, 4)\n",
      "(225600, 4)\n",
      "(234552, 6)\n",
      "(234552, 4)\n",
      "(234552, 4)\n",
      "(234552, 4)\n",
      "(5398, 2400, 6)\n",
      "(5398, 1)\n",
      "END read data\n"
     ]
    }
   ],
   "source": [
    "# 4 is the number of sessions per user\n",
    "train_set, train_label = create_datasets(0,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178064, 6)\n",
      "(178064, 4)\n",
      "(178064, 4)\n",
      "(178064, 4)\n",
      "(213868, 6)\n",
      "(213868, 4)\n",
      "(213868, 4)\n",
      "(213868, 4)\n",
      "(223204, 6)\n",
      "(223204, 4)\n",
      "(223204, 4)\n",
      "(223204, 4)\n",
      "(190148, 6)\n",
      "(190148, 4)\n",
      "(190148, 4)\n",
      "(190148, 4)\n",
      "(225860, 6)\n",
      "(225860, 4)\n",
      "(225860, 4)\n",
      "(225860, 4)\n",
      "(227188, 6)\n",
      "(227188, 4)\n",
      "(227188, 4)\n",
      "(227188, 4)\n",
      "(145364, 6)\n",
      "(145364, 4)\n",
      "(145364, 4)\n",
      "(145364, 4)\n",
      "(235188, 6)\n",
      "(235188, 4)\n",
      "(235188, 4)\n",
      "(235188, 4)\n",
      "(189588, 6)\n",
      "(189588, 4)\n",
      "(189588, 4)\n",
      "(189588, 4)\n",
      "(206336, 6)\n",
      "(206336, 4)\n",
      "(206336, 4)\n",
      "(206336, 4)\n",
      "(180316, 6)\n",
      "(180316, 4)\n",
      "(180316, 4)\n",
      "(180316, 4)\n",
      "(212684, 6)\n",
      "(212684, 4)\n",
      "(212684, 4)\n",
      "(212684, 4)\n",
      "(205104, 6)\n",
      "(205104, 4)\n",
      "(205104, 4)\n",
      "(205104, 4)\n",
      "(229924, 6)\n",
      "(229924, 4)\n",
      "(229924, 4)\n",
      "(229924, 4)\n",
      "(217200, 6)\n",
      "(217200, 4)\n",
      "(217200, 4)\n",
      "(217200, 4)\n",
      "(225776, 6)\n",
      "(225776, 4)\n",
      "(225776, 4)\n",
      "(225776, 4)\n",
      "(1386, 2400, 6)\n",
      "(1386, 1)\n",
      "END read data\n"
     ]
    }
   ],
   "source": [
    "test_set, test_label = create_datasets(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "43/43 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.5710\n",
      "Epoch 2/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5715\n",
      "Epoch 3/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5711\n",
      "Epoch 4/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5813\n",
      "Epoch 5/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5782\n",
      "Epoch 6/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5692\n",
      "Epoch 7/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5759\n",
      "Epoch 8/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5798\n",
      "Epoch 9/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5674\n",
      "Epoch 10/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5726\n",
      "Epoch 11/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5720\n",
      "Epoch 12/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5837\n",
      "Epoch 13/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5750\n",
      "Epoch 14/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5753\n",
      "Epoch 15/15\n",
      "43/43 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5732\n",
      "END network model\n"
     ]
    }
   ],
   "source": [
    "network_model(train_set,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
