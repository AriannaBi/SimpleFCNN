{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99038744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "import sys\n",
    "from functools import reduce\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import csv\n",
    "import glob\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "import keras \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from collections import Counter\n",
    "from keras.callbacks import History\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from tensorflow.keras.metrics import Precision\n",
    "# import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import CubicSpline      # for warping\n",
    "from transforms3d.axangles import axangle2mat  # for rotation\n",
    "import keras_tuner as kt\n",
    "from matplotlib import pyplot\n",
    "import xlsxwriter\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "np.set_printoptions(threshold=sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be1de258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## 1. Jittering\n",
    "\n",
    "# #### Hyperparameters :  sigma = standard devitation (STD) of the noise\n",
    "def DA_Jitter(X, sigma=0.01):\n",
    "    myNoise = np.random.normal(loc=0, scale=sigma, size=X.shape)\n",
    "    return X + myNoise\n",
    "\n",
    "\n",
    "# ## 2. Scaling\n",
    "\n",
    "# #### Hyperparameters :  sigma = STD of the zoom-in/out factor\n",
    "def DA_Scaling(X, sigma=0.5):\n",
    "    scalingFactor = np.random.normal(loc=1.0, scale=sigma, size=(1, X.shape[1]))  # shape=(1,3)\n",
    "    myNoise = np.matmul(np.ones((X.shape[0], 1)), scalingFactor)\n",
    "    return X * myNoise\n",
    "\n",
    "\n",
    "# ## 3. Magnitude Warping\n",
    "\n",
    "# #### Hyperparameters :  sigma = STD of the random knots for generating curves\n",
    "#\n",
    "# #### knot = # of knots for the random curves (complexity of the curves)\n",
    "\n",
    "# \"Scaling\" can be considered as \"applying constant noise to the entire samples\" whereas \"Jittering\" can be considered as \"applying different noise to each sample\".\n",
    "\n",
    "# \"Magnitude Warping\" can be considered as \"applying smoothly-varing noise to the entire samples\"\n",
    "\n",
    "\n",
    "## This example using cubic splice is not the best approach to generate random curves.\n",
    "## You can use other aprroaches, e.g., Gaussian process regression, Bezier curve, etc.\n",
    "def GenerateRandomCurves(X, sigma=0.2, knot=4):\n",
    "    xx = (np.ones((X.shape[1], 1)) * (np.arange(0, X.shape[0], (X.shape[0] - 1) / (knot + 1)))).transpose()\n",
    "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot + 2, X.shape[1]))\n",
    "    x_range = np.arange(X.shape[0])\n",
    "    random_curves = []\n",
    "    for i in range(X.shape[-1]):\n",
    "        cs = CubicSpline(xx[:, i], yy[:, i])\n",
    "        random_curves.append(cs(x_range))\n",
    "    return np.array(random_curves).transpose()\n",
    "\n",
    "\n",
    "def DA_MagWarp(X, sigma=0.2):\n",
    "    return X * GenerateRandomCurves(X, sigma)\n",
    "\n",
    "\n",
    "# ## 4. Time Warping\n",
    "\n",
    "# #### Hyperparameters :  sigma = STD of the random knots for generating curves\n",
    "#\n",
    "# #### knot = # of knots for the random curves (complexity of the curves)\n",
    "\n",
    "def DistortTimesteps(X, sigma=0.2):\n",
    "    tt = GenerateRandomCurves(X, sigma)  # Regard these samples aroun 1 as time intervals\n",
    "    tt_cum = np.cumsum(tt, axis=0)  # Add intervals to make a cumulative graph\n",
    "    # Make the last value to have X.shape[0]\n",
    "    for i in range(X.shape[-1]):\n",
    "        t_scale = (X.shape[0] - 1) / tt_cum[-1, i]\n",
    "        tt_cum[:, i] = tt_cum[:, i] * t_scale\n",
    "    return tt_cum\n",
    "\n",
    "\n",
    "def DA_TimeWarp(X, sigma=0.2):\n",
    "    tt_new = DistortTimesteps(X, sigma)\n",
    "    X_new = np.zeros(X.shape)\n",
    "    x_range = np.arange(X.shape[0])\n",
    "    for i in range(X.shape[-1]):\n",
    "        X_new[:, i] = np.interp(x_range, tt_new[:, i], X[:, i])\n",
    "    return X_new\n",
    "\n",
    "\n",
    "# ## 5. Permutation\n",
    "\n",
    "# #### Hyperparameters :  nPerm = # of segments to permute\n",
    "# #### minSegLength = allowable minimum length for each segment\n",
    "\n",
    "def DA_Permutation(X, nPerm=4, minSegLength=10):\n",
    "    X_new = np.zeros(X.shape)\n",
    "    idx = np.random.permutation(nPerm)\n",
    "    bWhile = True\n",
    "    while bWhile:\n",
    "        segs = np.zeros(nPerm + 1, dtype=int)\n",
    "        segs[1:-1] = np.sort(np.random.randint(minSegLength, X.shape[0] - minSegLength, nPerm - 1))\n",
    "        segs[-1] = X.shape[0]\n",
    "        if np.min(segs[1:] - segs[0:-1]) > minSegLength:\n",
    "            bWhile = False\n",
    "    pp = 0\n",
    "    for ii in range(nPerm):\n",
    "        x_temp = X[segs[idx[ii]]:segs[idx[ii] + 1], :]\n",
    "        X_new[pp:pp + len(x_temp), :] = x_temp\n",
    "        pp += len(x_temp)\n",
    "    return X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34e34be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def augment_data(train_set, train_label, function):\n",
    "\n",
    "    train_set_one = train_set\n",
    "#     START AUGMENTING\n",
    "    LABEL = []\n",
    "    # select random indices\n",
    "    number_of_rows = int(train_set_one.shape[0] * 0.5)\n",
    "\n",
    "#     random indices has to be the same for every dimension so that the label can be accurate\n",
    "    random_indices = np.sort(np.random.choice(train_set_one.shape[0]-1, size=int(number_of_rows), replace=False))\n",
    "    train_set_one = train_set_one[random_indices,:]\n",
    "    \n",
    "    \n",
    "    train_set_one = train_set_one.transpose()\n",
    "    if function == 'scale':\n",
    "        train_set_one = DA_Scaling(train_set_one)\n",
    "    elif function == 'jitter':\n",
    "        train_set_one = DA_Jitter(train_set_one)\n",
    "    elif function == 'magWarp':\n",
    "        train_set_one = DA_MagWarp(train_set_one)\n",
    "    elif function == 'timeWarp':\n",
    "        train_set_one = DA_TimeWarp(train_set_one)\n",
    "    elif function == 'rotation':\n",
    "        train_set_one = DA_Rotation(train_set_one)\n",
    "    elif function == 'permutation':\n",
    "        train_set_one = DA_Permutation(train_set_one)\n",
    "    else:\n",
    "        print(\"Error no augmentation function\")\n",
    "        return -1\n",
    "    train_set_one = train_set_one.transpose()\n",
    "        \n",
    "    \n",
    "    # take the label and add them as the label for the new augmented data\n",
    "    LABEL = np.array(train_label[random_indices])\n",
    "#     we have ARR which is of shape (6, row, col) with the augmented data\n",
    "#     and train_set which is of shape (6, row, col) with the non augmented data\n",
    "    \n",
    "    train_set_augmented = np.concatenate((train_set, train_set_one), axis = 0)\n",
    "    print(train_set[0,0])\n",
    "    print(train_set_one[0,0])\n",
    "    train_label = np.array(train_label)\n",
    "    label_set_augmented = np.concatenate((train_label, LABEL))\n",
    "    \n",
    "    return train_set_augmented, label_set_augmented\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1288c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_network(learning_rate, unit):\n",
    "    seed_value = 34567892\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "        \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(2400,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "#     model.add(Dense(128, activation='relu'))\n",
    "#     model.add(Dense(units=unit, activation='relu'))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     opt = SGD(learning_rate=learning_rate,momentum=0.5)\n",
    "    model.add(Dense(300, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = SGD(learning_rate=learning_rate,momentum=0.4)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy',Recall(), Precision()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb4324b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = np.load('train_set_original.npy',  encoding='ASCII')\n",
    "# train_label = np.load('train_label_original.npy',  encoding='ASCII')\n",
    "# test_set = np.load('test_set.npy',  encoding='ASCII')\n",
    "# test_label = np.load('test_label.npy',  encoding='ASCII')\n",
    "# train_label = train_label.reshape(train_label.shape[0], 1)\n",
    "# test_label = test_label.reshape(test_label.shape[0], 1)\n",
    "\n",
    "# # real parameters(0.0001, 40)\n",
    "\n",
    "# train_set_arr = train_set[0]\n",
    "# test_set_arr = test_set[0]\n",
    "# model=KerasClassifier(build_fn=model_network)\n",
    "# params={\"learning_rate\":[0.001, 0.0001, 0.00001], \"unit\":[16, 32, 64, 128]}\n",
    "# gs=GridSearchCV(estimator=model, param_grid=params,  cv=StratifiedShuffleSplit(n_splits=1))\n",
    "\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "# history = gs.fit(train_set_arr, train_label, epochs=50, batch_size= 128, shuffle=True, verbose=0, callbacks=[callback])\n",
    "\n",
    "# # callback = EarlyStopping(monitor='val_acc', patience=10, restore_best_weights=True)\n",
    "# # gs = gs.fit(x_train, y_train, epochs=500, batch_size= 128, callbacks = [callback])\n",
    "\n",
    "# print(\"Best: %f using %s\" % (gs.best_score_, gs.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fdc58b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 22:51:13.163762: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# for each technique run tec_len times the model to obtain an average of accuracy and fill the csv table with results\n",
    "best_lr = 0.1\n",
    "best_units = 100\n",
    "epoch = 100\n",
    "batch_s = 400\n",
    "\n",
    "techniques = ['jitter', 'scale', 'magWarp', 'timeWarp', 'permutation']\n",
    "\n",
    "line_csv_acc, line_csv_recall, line_csv_precision= [],[],[]\n",
    "std_accuracy, std_recall, std_precision  = [],[],[]\n",
    "arr_eda_percentage = []\n",
    "\n",
    "\n",
    "\n",
    "train_set = np.load('datasets/train_set_original.npy',  encoding='ASCII')\n",
    "train_label = np.load('datasets/train_label_original.npy',  encoding='ASCII')\n",
    "test_set = np.load('datasets/test_set.npy',  encoding='ASCII')\n",
    "test_label = np.load('datasets/test_label.npy',  encoding='ASCII')\n",
    "train_label = train_label.reshape(train_label.shape[0], 1)\n",
    "test_label = test_label.reshape(test_label.shape[0], 1)\n",
    "\n",
    "\n",
    "train_set_arr = train_set[1]\n",
    "test_set_arr = test_set[1]\n",
    "\n",
    "# model = model_network(0.0001, 40)\n",
    "model = model_network(best_lr, best_units)\n",
    "\n",
    "# ORIGINAL SET\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "history = model.fit(train_set_arr, train_label, epochs=epoch, batch_size= batch_s, shuffle=True, verbose=0, callbacks=[callback])\n",
    "scores1 = model.evaluate(test_set_arr, test_label, verbose=0)\n",
    "print(\"evaluate original: \",scores1)\n",
    "original_accuracy = scores1[1]\n",
    "line_csv_acc.append(round(scores1[1]* 100,4))\n",
    "line_csv_recall.append(round(scores1[2]* 100,4))\n",
    "line_csv_precision.append(round(scores1[3]* 100,4))\n",
    "std_accuracy.append(0)\n",
    "std_recall.append(0)\n",
    "std_precision.append(0)\n",
    "\n",
    "# pyplot.plot(history.history['loss'], label='train')\n",
    "# pyplot.legend()\n",
    "# pyplot.show()\n",
    "            \n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "\n",
    "for technique in techniques:\n",
    "    print(technique)\n",
    "    \n",
    "    recall_original = 0\n",
    "    accuracies, recalls, precisions = [],[],[]\n",
    "    \n",
    "    tec_len = 10\n",
    "    \n",
    "# loop tec_len times to get the average of a tecnique\n",
    "    for avg_t in range(0, tec_len):\n",
    "        train_set = np.load('datasets/train_set_original.npy',  encoding='ASCII')\n",
    "        train_label = np.load('datasets/train_label_original.npy',  encoding='ASCII')\n",
    "        test_set = np.load('datasets/test_set.npy',  encoding='ASCII')\n",
    "        test_label = np.load('datasets/test_label.npy',  encoding='ASCII')\n",
    "        train_label = train_label.reshape(train_label.shape[0], 1)\n",
    "        test_label = test_label.reshape(test_label.shape[0], 1)\n",
    "\n",
    "        \n",
    "        train_set_arr_augment, label_set_augmented = augment_data(train_set[1], train_label, technique)\n",
    "        \n",
    "        test_set_arr = test_set[1]\n",
    "        model = model_network(best_lr, best_units)\n",
    "\n",
    "        # AUGMENTATION\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "        history = model.fit(train_set_arr_augment, label_set_augmented, epochs=epoch, batch_size= batch_s, shuffle=True,verbose=0, callbacks = [callback])\n",
    "        scores2 = model.evaluate(test_set_arr, test_label,verbose=0)\n",
    "        #     --------------------------------------------------------------------\n",
    "        print(\"evaluate augmented : \", technique, scores2)\n",
    "\n",
    "        accuracies.append(scores2[1])\n",
    "        recalls.append(scores2[2])\n",
    "        precisions.append(scores2[3])\n",
    "\n",
    "        \n",
    "#         pyplot.plot(history.history['loss'], label='train')\n",
    "#         pyplot.legend()\n",
    "#         pyplot.show()\n",
    " \n",
    "\n",
    "    line_csv_acc.append(round(np.mean(accuracies)* 100,4))\n",
    "    line_csv_recall.append(round(np.mean(recalls)* 100,4))\n",
    "    line_csv_precision.append(round(np.mean(precisions)* 100,4))\n",
    "    std_accuracy.append(round(np.std(accuracies)* 100,4))\n",
    "    std_recall.append(round(np.std(recalls)* 100,4))\n",
    "    std_precision.append(round(np.std(precisions)* 100,4))\n",
    "    print(accuracies)\n",
    "    print(recalls)\n",
    "    print(precisions)\n",
    "    print(\"mean: {} -- std: (+/- {}\".format(np.mean(accuracies), np.std(accuracies)))\n",
    "\n",
    "    arr_eda_percentage.append(round((np.mean(accuracies) - original_accuracy) * 100, 4))\n",
    "\n",
    "\n",
    "\n",
    "line_csv_acc.insert(0, 'BVP')\n",
    "std_accuracy.insert(0, 'STD ACCURACY')\n",
    "line_csv_recall.insert(0, 'RECALL')\n",
    "std_recall.insert(0, 'STD RECALL')\n",
    "line_csv_precision.insert(0, 'PRECISION')\n",
    "std_precision.insert(0, 'STD PRECISION')\n",
    "\n",
    "\n",
    "\n",
    "header = ['sensor', 'baseline', 'jitter', 'scale', 'magWarp', 'timeWarp', 'permutation']\n",
    "\n",
    "with open('table_accuracy_BVP.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerow(line_csv_acc)\n",
    "    writer.writerow(std_accuracy)\n",
    "    writer.writerow(line_csv_recall)\n",
    "    writer.writerow(std_recall)\n",
    "    writer.writerow(line_csv_precision)\n",
    "    writer.writerow(std_precision)\n",
    "\n",
    "    writer.writerow([])\n",
    "    arr_eda_percentage.insert(0, 0)\n",
    "    arr_eda_percentage.insert(0, 'BVP')\n",
    "    writer.writerow(arr_eda_percentage)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72007ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b02ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
