{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99038744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "import sys\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import csv\n",
    "import glob\n",
    "import keras \n",
    "from collections import Counter\n",
    "from tempfile import TemporaryFile\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74f65436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a path like \"../trial/S01/ACC_01.parquet\" return:\n",
    "# the number of ms that a file needs in order to do the downsampling;\n",
    "# and return the type among:\n",
    "# - 0 (EDA)\n",
    "# - 1 (BVP)\n",
    "# - 2 (ACC)\n",
    "# - 3 (TEM) \n",
    "\n",
    "# If the file is incorrect return -1\n",
    "def type_file(name):\n",
    "    name_file = name.split('/')[3][:3]\n",
    "    if name_file == 'EDA':\n",
    "        return 0, 0\n",
    "    elif name_file == 'BVP':\n",
    "        return '250ms', 1\n",
    "    elif name_file == 'ACC':\n",
    "        return '250ms', 2\n",
    "    elif name_file == 'TEM':\n",
    "        return 0, 5\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aaa7661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the most frequent element in a list:\n",
    "# [0,0,0,1,1] return 0\n",
    "# [0,0,1,1,1] return 1\n",
    "# listt = [1,1,1,1,0,0,0]\n",
    "def most_frequent(listt):\n",
    "#     print(listt)\n",
    "    listt = sorted(listt)\n",
    "#     print(listt)\n",
    "    counter = 0\n",
    "    \n",
    "    num = listt[0]\n",
    "\n",
    "    for i in listt:\n",
    "        curr_frequency = listt.count(i)\n",
    "        if(curr_frequency> counter):\n",
    "            counter = curr_frequency\n",
    "            num = i\n",
    "    return num\n",
    "\n",
    "# listt = [1, 0,0,1,0,1]\n",
    "# print(most_frequent(listt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f6fe9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a data frame that contains the data, create ARRAY filling it with the data in the data frame. \n",
    "# We need ARRAY to be a multiple of 2400 so that we can create rows of 2400 cols. Hence add the missing \n",
    "# data (np.nan) at the end of ARRAY. \n",
    "# Example: ARRAY.len = 1390, meaning we need to add (2400 - 1390) 10 nan at the end of ARRAY \n",
    "def append_data_and_fill_missing_nan(ARRAY, df, value):\n",
    "    for i in range(0, df.shape[0]):\n",
    "        ARRAY.append(df[value][i])\n",
    "\n",
    "    if (len(ARRAY) % 2400) != 0:\n",
    "        rounding = int(len(ARRAY)//2400 + (len(ARRAY) % 2400 > 0))\n",
    "        number_cells = rounding * 2400\n",
    "        missing_zeros = [np.nan] * (number_cells - len(ARRAY))\n",
    "        ARRAY = ARRAY + missing_zeros\n",
    "    return ARRAY\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c231d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sleep_label(LABEL, EDA, df):\n",
    "    i = 0\n",
    "    list_sleep = []\n",
    "#     create the list_sleep with all the values of the colums 'Sleep'\n",
    "    for j in range(0, df.shape[0]):\n",
    "        value = df['Sleep'][j].astype(np.float64)\n",
    "        list_sleep.append(value)\n",
    "\n",
    "#   for each 2400 window choose if 0 or 1, and append it to ARRAR_LABEL\n",
    "    while i in range(0, len(list_sleep)- 2399):\n",
    "        list_sleep_window = list_sleep[i:(i+2400)]\n",
    "        label_sleep = most_frequent(list_sleep_window)\n",
    "        LABEL.append(label_sleep)\n",
    "        i += 2400\n",
    "        \n",
    "#     if EDA len is greather than ARRAY_LABEL, uniform it to the same length\n",
    "    if (len(LABEL) < int(len(EDA)/2400)):\n",
    "        missing_zeros = [np.nan]*(int(len(EDA)/2400)-len(LABEL))\n",
    "        LABEL = LABEL + missing_zeros\n",
    "    return LABEL\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e1b4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zeros_to_uniform_size_array(ARRAY, missing_zeros):\n",
    "    ARRAY = [np.nan]* missing_zeros\n",
    "    return ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f138719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_arrays(ARRAY, ARR2, ARR3, ARR4, ARR5, ARR6, LABEL):\n",
    "    array_zeros = np.zeros(2400)\n",
    "    \n",
    "    array_boolean = np.isnan(ARRAY).any(axis=1)\n",
    "\n",
    "    # iterate over each row.\n",
    "    # if delete a \"true\" then continue in a while untill it goes 1 step forward\n",
    "    i = 0\n",
    "    while i < len(array_boolean):\n",
    "        while array_boolean[i]:\n",
    "            ARRAY = np.delete(ARRAY, i, 0)\n",
    "            ARR2 = np.delete(ARR2, i, 0)\n",
    "            ARR3 = np.delete(ARR3, i, 0)\n",
    "            ARR4 = np.delete(ARR4, i, 0)\n",
    "            ARR5 = np.delete(ARR5, i, 0)\n",
    "            ARR6 = np.delete(ARR6, i, 0)\n",
    "            LABEL = np.delete(LABEL, i, 0)\n",
    "            array_boolean = np.delete(array_boolean, i)\n",
    "        i += 1\n",
    "    \n",
    "    ARRAY = ARRAY[:-1]\n",
    "    ARR2 = ARR2[:-1]\n",
    "    ARR3 = ARR3[:-1]\n",
    "    ARR4 = ARR4[:-1]\n",
    "    ARR5 = ARR5[:-1]\n",
    "    ARR6 = ARR6[:-1]\n",
    "    LABEL = LABEL[:-1]\n",
    "    return ARRAY, ARR2, ARR3, ARR4, ARR5, ARR6, LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc53c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from array 1D, i create an array 2D with the last row as 0.\n",
    "def reshape_arrays(EDA):\n",
    "    EDA = np.array(EDA)\n",
    "#     print(\"eda before: \",EDA.shape)\n",
    "    EDA = EDA.reshape(1,len(EDA)//2400, 2400)\n",
    "    array_zeros = np.zeros(2400)\n",
    "    EDA = EDA[0]\n",
    "    EDA = np.concatenate((EDA, [array_zeros]), axis=0)\n",
    "    \n",
    "#     print(\"eda after: \",EDA.shape)\n",
    "    return EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1e6ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from array 1D, i append a 0 in the last row.\n",
    "def reshape_label(LABEL):\n",
    "    LABEL = np.array(LABEL)\n",
    "#     print(\"LABEL before: \", LABEL.shape)\n",
    "#     LABEL = LABEL.reshape(1,len(LABEL), 1)\n",
    "#     array_zeros = np.zeros(2400)\n",
    "#     LABEL = LABEL[0]\n",
    "    LABEL = np.concatenate((LABEL, [0]), axis=0)\n",
    "    \n",
    "#     print(\"LABEL after: \",LABEL.shape)\n",
    "    return LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a63171cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call function create_train_set_train_label so that we have the two zero tensors.\n",
    "# for each file read the data, downsampling the data and fill the tensors.\n",
    "\n",
    "def read_data(files):\n",
    "    EDA, BVP, ACC_X, ACC_Y, ACC_Z, TEM, LABEL = [],[],[],[],[],[],[]\n",
    "\n",
    "\n",
    "    # READ ALL THE FILES and DOWNSAMPLING\n",
    "    for name_file in files:\n",
    "#         print(name_file)\n",
    "        rounding, number_cells, missing_zeros = 0, 0, []\n",
    "        # DataFrame\n",
    "        table = pd.read_parquet(name_file, engine='pyarrow')\n",
    "        # creating DataFrame\n",
    "        df = pd.DataFrame(table)\n",
    "        # converting timestamp\n",
    "        timestamp_col = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        df['timestamp'] = timestamp_col\n",
    "        # get if file it's among EDA, BVP, ACC, ST, otherwise return error\n",
    "        arr_type = type_file(name_file)\n",
    "        if arr_type == -1:\n",
    "            print(\"Error in the file name\")\n",
    "            return -1\n",
    "\n",
    "        # RESAMPLE the tensor\n",
    "        # 0 means it's already 4Hz, otherwise resample with '250ms'\n",
    "        if arr_type[0] != 0:\n",
    "            df = df.resample(arr_type[0], on='timestamp').mean()\n",
    "            df = df.reset_index()\n",
    "              \n",
    "        \n",
    "       # CREATES THE ARRAYS\n",
    "        # create linear arrays as sequences of elements. for each file add several zeros to complete a window of 2400     \n",
    "        if arr_type[1] == 0:\n",
    "            EDA = append_data_and_fill_missing_nan(EDA, df, 'value')\n",
    "            LABEL = find_sleep_label(LABEL, EDA, df)\n",
    "                \n",
    "        elif arr_type[1] == 1:\n",
    "            BVP = append_data_and_fill_missing_nan(BVP, df, 'value')\n",
    "\n",
    "        elif arr_type[1] == 2:\n",
    "            ACC_X = append_data_and_fill_missing_nan(ACC_X, df, 'X')\n",
    "            ACC_Y = append_data_and_fill_missing_nan(ACC_Y, df, 'Y')\n",
    "            ACC_Z = append_data_and_fill_missing_nan(ACC_Z, df, 'Z')\n",
    "            \n",
    "        elif arr_type[1] == 5:\n",
    "            TEM = append_data_and_fill_missing_nan(TEM, df, 'value')\n",
    "        \n",
    "        \n",
    "#     if each file has the same length it doesn make sense to check it. only in between files fill the row of zeros\n",
    "#     and then start from the next row a new file\n",
    "#     if the files are not the same quantity, it's better to throw error?\n",
    "#     uniform_len = max(len(EDA),len(BVP),len(ACC_X),len(ACC_Y),len(ACC_Z),len(TEM))  \n",
    "    if len(EDA) != len(BVP) != len(ACC_X) != len(ACC_Y) != len(ACC_Z) != len(TEM):\n",
    "        print(\"Error in the file name\")\n",
    "        return -1\n",
    "#     print(len(EDA), ' ',len(BVP), ' ',len(ACC_X), ' ',len(ACC_Y), ' ',len(ACC_Z), ' ',len(TEM) )\n",
    "    \n",
    "#     if len(EDA) != uniform_len:\n",
    "#         EDA = EDA + [np.nan]* (uniform_len - len(EDA))\n",
    "#     if len(BVP) != uniform_len:\n",
    "#         BVP = BVP + [np.nan]* (uniform_len - len(BVP))\n",
    "#     if len(ACC_X) != uniform_len:\n",
    "#         ACC_X = ACC_X + [np.nan]* (uniform_len - len(ACC_X))\n",
    "#     if len(ACC_Y) != uniform_len:\n",
    "#         ACC_Y = ACC_Y + [np.nan]* (uniform_len - len(ACC_Y))\n",
    "#     if len(ACC_Z) != uniform_len:\n",
    "#         ACC_Z = ACC_Z + [np.nan]* (uniform_len - len(ACC_Z))\n",
    "#     if len(TEM) != uniform_len:\n",
    "#         TEM = TEM + [np.nan]* (uniform_len - len(TEM))\n",
    "        \n",
    "#     print(len(EDA_LABEL), ' ',len(BVP_LABEL), ' ',len(ACC_X_LABEL), ' ',len(ACC_Y_LABEL), ' ',len(ACC_Z_LABEL), ' ',len(TEM_LABEL) )\n",
    "    \n",
    "    \n",
    "    # convert the list to a numpy array\n",
    "#     removing rows with nan\n",
    "    EDA_np, BVP_np, TEM_np = np.array(EDA),np.array(BVP),np.array(TEM)\n",
    "    ACC_X_np, ACC_Y_np, ACC_Z_np = np.array(ACC_X),np.array(ACC_Y),np.array(ACC_Z)\n",
    "    LABEL_np = np.array(LABEL)\n",
    "\n",
    "    \n",
    "    EDA_np = reshape_arrays(EDA)\n",
    "    BVP_np = reshape_arrays(BVP)\n",
    "    ACC_X_np = reshape_arrays(ACC_X)\n",
    "    ACC_Y_np = reshape_arrays(ACC_Y)\n",
    "    ACC_Z_np = reshape_arrays(ACC_Z)\n",
    "    TEM_np = reshape_arrays(TEM)\n",
    "    LABEL_np =  reshape_label(LABEL)\n",
    "\n",
    "    \n",
    "\n",
    "    EDA_np, BVP_np, ACC_X_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np = remove_nan_arrays(EDA_np, BVP_np, ACC_X_np, ACC_Y_np, ACC_Z_np, TEM_np, LABEL_np)\n",
    "# each file has different nan rows\n",
    "    BVP_np, EDA_np, ACC_X_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np = remove_nan_arrays(BVP_np, EDA_np, ACC_X_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np)\n",
    "    ACC_X_np, EDA_np, BVP_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np = remove_nan_arrays(ACC_X_np, EDA_np, BVP_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np)\n",
    "    ACC_Y_np, EDA_np, BVP_np, ACC_X_np, ACC_Z_np, TEM_np,LABEL_np = remove_nan_arrays(ACC_Y_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, TEM_np,LABEL_np)\n",
    "    ACC_Z_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, TEM_np,LABEL_np = remove_nan_arrays(ACC_Z_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, TEM_np,LABEL_np)\n",
    "    TEM_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, ACC_Z_np,LABEL_np = remove_nan_arrays(TEM_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, ACC_Z_np,LABEL_np)\n",
    "\n",
    "\n",
    "\n",
    "    EDA = EDA_np.flatten().tolist()\n",
    "    BVP = BVP_np.flatten().tolist()\n",
    "    ACC_X = ACC_X_np.flatten().tolist()\n",
    "    ACC_Y = ACC_Y_np.flatten().tolist()\n",
    "    ACC_Z = ACC_Z_np.flatten().tolist()\n",
    "    TEM = TEM_np.flatten().tolist()\n",
    "    \n",
    "    \n",
    "    train_set = np.array([EDA, BVP, ACC_X, ACC_Y, ACC_Z, TEM],dtype=np.float64)\n",
    "#     print(\"LABELLLLLLL\")\n",
    "#     print(LABEL_np.shape)\n",
    "    train_label = LABEL_np\n",
    "#     print(train_label)\n",
    "#     train_label = np.array([LABEL_np], dtype=np.float64)\n",
    "    \n",
    "\n",
    "    # Reshape the tensor train set, counting the number of rows dividing by 2400\n",
    "    uniform_len = max(len(EDA),len(BVP),len(ACC_X),len(ACC_Y),len(ACC_Z),len(TEM))    \n",
    "    row_training_set = uniform_len // 2400\n",
    "    if (uniform_len % 2400) != 0:\n",
    "        return \"Error in missing_zeros\"\n",
    "    \n",
    "    train_set = train_set.reshape(6,row_training_set,2400)\n",
    "    \n",
    "    print(\"creating dataset\")\n",
    "    print(train_set.shape)\n",
    "    print(train_label.shape)\n",
    "    print(\"end dataset\")\n",
    "    \n",
    "    # compat the ACC_X, ACC_Y, ACC_Z to ACC with formula √(x*x + y*y + z*z)\n",
    "    acc_x_squared = np.multiply(train_set[2], train_set[2])\n",
    "    acc_y_squared = np.multiply(train_set[3], train_set[3])\n",
    "    acc_z_squared = np.multiply(train_set[4], train_set[4])\n",
    "    acc_xyz_sum = acc_x_squared + acc_y_squared + acc_z_squared\n",
    "    acc = np.sqrt(acc_xyz_sum)\n",
    "    \n",
    "#     reconstruct train_set with EDA, BVC, ACC, TEM (4 dimensions instead of 6)\n",
    "    train_set = np.array([train_set[0], train_set[1], acc, train_set[5]])\n",
    "    \n",
    "    \n",
    "    return train_set, train_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba979397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(start_session, end_session):\n",
    "#     create list of folders\n",
    "    listt = os.listdir(\"../Sessions/\")\n",
    "#     print(listt)\n",
    "#   how many users? for now only 1\n",
    "#     list_dir = sorted(listt)[0:1]\n",
    "    list_dir = sorted(listt)\n",
    "    print(list_dir)\n",
    "    list_path_dir = []\n",
    "    for elem in list_dir:\n",
    "        list_path_dir.append(\"../Sessions/\" + elem)\n",
    "\n",
    "#     read files in the folder and create a list of files \n",
    "    list_ordered_files = []\n",
    "    \n",
    "    for folder in list_path_dir:\n",
    "#         print(folder)\n",
    "        list_files = []\n",
    "        for infile in os.listdir(folder):\n",
    "            list_files.append(infile)\n",
    "        list_files = sorted(list_files)\n",
    "#         print(list_files)\n",
    "\n",
    "    #     divide the list into 4 sections: acc, bvp, eda, temp\n",
    "    #     take one from each and construct a list \n",
    "    #     the order is really important while reading files\n",
    "        \n",
    "        acc = []\n",
    "        bvp = []\n",
    "        eda = []\n",
    "        tem = []\n",
    "        for elem in (list_files):\n",
    "            if elem[:3] == \"ACC\":\n",
    "                acc.append(elem)\n",
    "            elif elem[:3] == \"BVP\":\n",
    "                bvp.append(elem)\n",
    "            elif elem[:3] == \"EDA\":\n",
    "                eda.append(elem)\n",
    "            elif elem[:3] == \"TEM\":\n",
    "                tem.append(elem)\n",
    "\n",
    "        acc = acc[start_session:end_session]\n",
    "        bvp = bvp[start_session:end_session]\n",
    "        eda = eda[start_session:end_session]\n",
    "        tem = tem[start_session:end_session]\n",
    "    #     create a list with that order: [acc1, bvp1, eda1, tem1, acc2, bvp2, eda2, tem2,...]\n",
    "        for i in range(len(acc)):\n",
    "            list_ordered_files.append(folder + \"/\" + acc[i])\n",
    "            list_ordered_files.append(folder + \"/\" + bvp[i])\n",
    "            list_ordered_files.append(folder + \"/\" + eda[i])\n",
    "            list_ordered_files.append(folder + \"/\" + tem[i])\n",
    "    print(len(list_ordered_files))\n",
    "        \n",
    "    \n",
    "    return read_data(list_ordered_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a3fcf96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S01', 'S02', 'S03', 'S04', 'S05', 'S06', 'S07', 'S08', 'S09', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16']\n",
      "840\n",
      "creating dataset\n",
      "(6, 16231, 2400)\n",
      "(16231,)\n",
      "end dataset\n"
     ]
    }
   ],
   "source": [
    "# 4 is the number of sessions per user\n",
    "# train_set, train_label = read_files(0,4)\n",
    "# from 0 session to 5 session per user\n",
    "train_set_original, train_label_original =  create_datasets(0,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "811b03a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16231, 2400)\n"
     ]
    }
   ],
   "source": [
    "np.save('train_set_original', train_set_original)\n",
    "arr1 = np.load('train_set_original.npy',  encoding='ASCII')\n",
    "print(arr1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0550427a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16231,)\n"
     ]
    }
   ],
   "source": [
    "np.save('train_label_original', train_label_original)\n",
    "arr2 = np.load('train_label_original.npy',  encoding='ASCII')\n",
    "print(arr2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d159b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a758cb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S01', 'S02', 'S03', 'S04', 'S05', 'S06', 'S07', 'S08', 'S09', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16']\n",
      "112\n",
      "creating dataset\n",
      "(6, 2231, 2400)\n",
      "(2231,)\n",
      "end dataset\n"
     ]
    }
   ],
   "source": [
    "test_set, test_label = create_datasets(15,17)\n",
    "# test_label = test_label.reshape(test_label.shape[1], 1)\n",
    "# print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "520663b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2231, 2400)\n"
     ]
    }
   ],
   "source": [
    "np.save('test_set', test_set)\n",
    "arr3 = np.load('test_set.npy',  encoding='ASCII')\n",
    "print(arr3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67c620ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2231,)\n"
     ]
    }
   ],
   "source": [
    "np.save('test_label', test_label)\n",
    "arr4 = np.load('test_label.npy',  encoding='ASCII')\n",
    "print(arr4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362d2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
