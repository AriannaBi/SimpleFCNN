{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c05d38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "# random seed for numpy\n",
    "np.random.seed(37)\n",
    "# random seed for python\n",
    "rn.seed(1254)\n",
    "# random seed for tensorflow\n",
    "tf.random.set_seed(89)\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "# from itertools import chain\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import keras \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import History\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c9667c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a path like \"../trial/S01/ACC_01.parquet\" return:\n",
    "# the number of ms that a file needs in order to do the downsampling;\n",
    "# and return the type among:\n",
    "# - 0 (EDA)\n",
    "# - 1 (BVP)\n",
    "# - 2 (ACC)\n",
    "# - 3 (TEM) \n",
    "\n",
    "# If the file is incorrect return -1\n",
    "def type_file(name):\n",
    "    name_file = name.split('/')[3][:3]\n",
    "#     print(name_file)\n",
    "    if name_file == 'EDA':\n",
    "        return 0, 0\n",
    "    elif name_file == 'BVP':\n",
    "        return '250ms', 1\n",
    "    elif name_file == 'ACC':\n",
    "        return '250ms', 2\n",
    "    elif name_file == 'TEM':\n",
    "        return 0, 5\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bf226be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the most frequent element in a list:\n",
    "# [0,0,0,1,1] return 0\n",
    "# [0,0,1,1,1] return 1\n",
    "\n",
    "def most_frequent(List):\n",
    "    counter = 0\n",
    "    num = List[0]\n",
    "\n",
    "    for i in List:\n",
    "        curr_frequency = List.count(i)\n",
    "        if(curr_frequency> counter):\n",
    "            counter = curr_frequency\n",
    "            num = i\n",
    "    return num\n",
    "\n",
    "# listt = [0,1,0,1]\n",
    "# print(most_frequent(listt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "677c31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data_and_fill_missing_nan(ARRAY, df, value):\n",
    "    for i in range(0, df.shape[0]):\n",
    "        ARRAY.append(df[value][i])\n",
    "\n",
    "    if (len(ARRAY) % 2400) != 0:\n",
    "        rounding = int(len(ARRAY)//2400 + (len(ARRAY) % 2400 > 0))\n",
    "        number_cells = rounding * 2400\n",
    "        missing_zeros = [np.nan] * (number_cells - len(ARRAY))\n",
    "        ARRAY = ARRAY + missing_zeros\n",
    "    return ARRAY\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cd384033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sleep_label(LABEL, EDA, df):\n",
    "    i = 0\n",
    "    list_sleep = []\n",
    "#     create the list_sleep with all the values of the colums 'Sleep'\n",
    "    for j in range(0, df.shape[0]):\n",
    "        value = df['Sleep'][j].astype(np.float64)\n",
    "        list_sleep.append(value)\n",
    "\n",
    "#   for each 2400 window choose if 0 or 1, and append it to ARRAR_LABEL\n",
    "    while i in range(0, len(list_sleep)- 2399):\n",
    "        list_sleep_window = list_sleep[i:(i+2400)]\n",
    "        label_sleep = most_frequent(list_sleep_window)\n",
    "        LABEL.append(label_sleep)\n",
    "        i += 2400\n",
    "        \n",
    "#     if EDA len is greather than ARRAY_LABEL, uniform it to the same length\n",
    "    if (len(LABEL) < int(len(EDA)/2400)):\n",
    "        missing_zeros = [np.nan]*(int(len(EDA)/2400)-len(LABEL))\n",
    "        LABEL = LABEL + missing_zeros\n",
    "        \n",
    "#     print(len(LABEL))\n",
    "    return LABEL\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d642c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zeros_to_uniform_size_array(ARRAY, missing_zeros):\n",
    "    ARRAY = [np.nan]* missing_zeros\n",
    "    return ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "99d19b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_arrays(ARRAY, ARR2, ARR3, ARR4, ARR5, ARR6, LABEL):\n",
    "    array_zeros = np.zeros(2400)\n",
    "    \n",
    "    array_boolean = np.isnan(ARRAY).any(axis=1)\n",
    "\n",
    "    # iterate over each row.\n",
    "    # if delete a \"true\" then continue in a while untill it goes 1 step forward\n",
    "    i = 0\n",
    "    while i < len(array_boolean):\n",
    "        while array_boolean[i]:\n",
    "            ARRAY = np.delete(ARRAY, i, 0)\n",
    "            ARR2 = np.delete(ARR2, i, 0)\n",
    "            ARR3 = np.delete(ARR3, i, 0)\n",
    "            ARR4 = np.delete(ARR4, i, 0)\n",
    "            ARR5 = np.delete(ARR5, i, 0)\n",
    "            ARR6 = np.delete(ARR6, i, 0)\n",
    "            LABEL = np.delete(LABEL, i, 0)\n",
    "            array_boolean = np.delete(array_boolean, i)\n",
    "        i += 1\n",
    "    \n",
    "#     print(ARRAY.shape)\n",
    "    ARRAY = ARRAY[:-1]\n",
    "    ARR2 = ARR2[:-1]\n",
    "    ARR3 = ARR3[:-1]\n",
    "    ARR4 = ARR4[:-1]\n",
    "    ARR5 = ARR5[:-1]\n",
    "    ARR6 = ARR6[:-1]\n",
    "    LABEL = LABEL[:-1]\n",
    "    return ARRAY, ARR2, ARR3, ARR4, ARR5, ARR6, LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1b725831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from array 1D, i create an array 2D with the last row as 0.\n",
    "def reshape_arrays(EDA):\n",
    "    EDA = np.array(EDA)\n",
    "#     print(\"eda before: \",EDA.shape)\n",
    "    EDA = EDA.reshape(1,len(EDA)//2400, 2400)\n",
    "    array_zeros = np.zeros(2400)\n",
    "    EDA = EDA[0]\n",
    "    EDA = np.concatenate((EDA, [array_zeros]), axis=0)\n",
    "    \n",
    "#     print(\"eda after: \",EDA.shape)\n",
    "    return EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b3fb945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from array 1D, i append a 0 in the last row.\n",
    "def reshape_label(LABEL):\n",
    "    LABEL = np.array(LABEL)\n",
    "#     print(\"LABEL before: \", LABEL.shape)\n",
    "#     LABEL = LABEL.reshape(1,len(LABEL), 1)\n",
    "#     array_zeros = np.zeros(2400)\n",
    "#     LABEL = LABEL[0]\n",
    "    LABEL = np.concatenate((LABEL, [0]), axis=0)\n",
    "    \n",
    "#     print(\"LABEL after: \",LABEL.shape)\n",
    "    return LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "20e2fcb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call function create_train_set_train_label so that we have the two zero tensors.\n",
    "# for each file read the data, downsampling the data and fill the tensors.\n",
    "\n",
    "def read_data(files):\n",
    "    EDA, BVP, ACC_X, ACC_Y, ACC_Z, TEM, LABEL = [],[],[],[],[],[],[]\n",
    "\n",
    "\n",
    "    # READ ALL THE FILES and DOWNSAMPLING\n",
    "    for name_file in files:\n",
    "#         print(name_file)\n",
    "        rounding, number_cells, missing_zeros = 0, 0, []\n",
    "        # DataFrame\n",
    "        table = pd.read_parquet(name_file, engine='pyarrow')\n",
    "        # creating DataFrame\n",
    "        df = pd.DataFrame(table)\n",
    "        # converting timestamp\n",
    "        timestamp_col = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        df['timestamp'] = timestamp_col\n",
    "        # get if file it's among EDA, BVP, ACC, ST, otherwise return error\n",
    "        arr_type = type_file(name_file)\n",
    "        if arr_type == -1:\n",
    "            print(\"Error in the file name\")\n",
    "            return -1\n",
    "\n",
    "        # RESAMPLE the tensor\n",
    "        # 0 means it's already 4Hz, otherwise resample with '250ms'\n",
    "        if arr_type[0] != 0:\n",
    "            df = df.resample(arr_type[0], on='timestamp').mean()\n",
    "            df = df.reset_index()\n",
    "        \n",
    "        # remove the          \n",
    "        \n",
    "#         print(df.shape)\n",
    "       # CREATES THE ARRAYS\n",
    "        # create linear arrays as sequences of elements. for each file add several zeros to complete a window of 2400     \n",
    "        if arr_type[1] == 0:\n",
    "            EDA = append_data_and_fill_missing_nan(EDA, df, 'value')\n",
    "            LABEL = find_sleep_label(LABEL, EDA, df)\n",
    "                \n",
    "        elif arr_type[1] == 1:\n",
    "            BVP = append_data_and_fill_missing_nan(BVP, df, 'value')\n",
    "\n",
    "        elif arr_type[1] == 2:\n",
    "            ACC_X = append_data_and_fill_missing_nan(ACC_X, df, 'X')\n",
    "            ACC_Y = append_data_and_fill_missing_nan(ACC_Y, df, 'Y')\n",
    "            ACC_Z = append_data_and_fill_missing_nan(ACC_Z, df, 'Z')\n",
    "            \n",
    "        elif arr_type[1] == 5:\n",
    "            TEM = append_data_and_fill_missing_nan(TEM, df, 'value')\n",
    "        \n",
    "        \n",
    "#     print(\"LEN LABEL\",len(LABEL))\n",
    "#     if each file has the same length it doesn make sense to check it. only in between files fill the row of zeros\n",
    "#     and then start from the next row a new file\n",
    "#     if the files are not the same quantity, it's better to throw error?\n",
    "    uniform_len = max(len(EDA),len(BVP),len(ACC_X),len(ACC_Y),len(ACC_Z),len(TEM))    \n",
    "#     print(len(EDA), ' ',len(BVP), ' ',len(ACC_X), ' ',len(ACC_Y), ' ',len(ACC_Z), ' ',len(TEM) )\n",
    "#     check train_set\n",
    "    if len(EDA) != uniform_len:\n",
    "        EDA = EDA + [np.nan]* (uniform_len - len(EDA))\n",
    "    if len(BVP) != uniform_len:\n",
    "        BVP = BVP + [np.nan]* (uniform_len - len(BVP))\n",
    "    if len(ACC_X) != uniform_len:\n",
    "        ACC_X = ACC_X + [np.nan]* (uniform_len - len(ACC_X))\n",
    "    if len(ACC_Y) != uniform_len:\n",
    "        ACC_Y = ACC_Y + [np.nan]* (uniform_len - len(ACC_Y))\n",
    "    if len(ACC_Z) != uniform_len:\n",
    "        ACC_Z = ACC_Z + [np.nan]* (uniform_len - len(ACC_Z))\n",
    "    if len(TEM) != uniform_len:\n",
    "        TEM = TEM + [np.nan]* (uniform_len - len(TEM))\n",
    "        \n",
    "#     print(len(EDA_LABEL), ' ',len(BVP_LABEL), ' ',len(ACC_X_LABEL), ' ',len(ACC_Y_LABEL), ' ',len(ACC_Z_LABEL), ' ',len(TEM_LABEL) )\n",
    "    \n",
    "    \n",
    "    # convert the list to a numpy array\n",
    "#     removing rows with nan\n",
    "    EDA_np, BVP_np, TEM_np = np.array(EDA),np.array(BVP),np.array(TEM)\n",
    "    ACC_X_np, ACC_Y_np, ACC_Z_np = np.array(ACC_X),np.array(ACC_Y),np.array(ACC_Z)\n",
    "    LABEL_np = np.array(LABEL)\n",
    "\n",
    "    \n",
    "    EDA_np = reshape_arrays(EDA)\n",
    "    BVP_np = reshape_arrays(BVP)\n",
    "    ACC_X_np = reshape_arrays(ACC_X)\n",
    "    ACC_Y_np = reshape_arrays(ACC_Y)\n",
    "    ACC_Z_np = reshape_arrays(ACC_Z)\n",
    "    TEM_np = reshape_arrays(TEM)\n",
    "    LABEL_np =  reshape_label(LABEL)\n",
    "\n",
    "    \n",
    "\n",
    "    EDA_np, BVP_np, ACC_X_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np = remove_nan_arrays(EDA_np, BVP_np, ACC_X_np, ACC_Y_np, ACC_Z_np, TEM_np, LABEL_np)\n",
    "# each file has different nan rows\n",
    "    BVP_np, EDA_np, ACC_X_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np = remove_nan_arrays(BVP_np, EDA_np, ACC_X_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np)\n",
    "    ACC_X_np, EDA_np, BVP_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np = remove_nan_arrays(ACC_X_np, EDA_np, BVP_np, ACC_Y_np, ACC_Z_np, TEM_np,LABEL_np)\n",
    "    ACC_Y_np, EDA_np, BVP_np, ACC_X_np, ACC_Z_np, TEM_np,LABEL_np = remove_nan_arrays(ACC_Y_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, TEM_np,LABEL_np)\n",
    "    ACC_Z_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, TEM_np,LABEL_np = remove_nan_arrays(ACC_Z_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, TEM_np,LABEL_np)\n",
    "    TEM_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, ACC_Z_np,LABEL_np = remove_nan_arrays(TEM_np, EDA_np, BVP_np, ACC_X_np, ACC_Y_np, ACC_Z_np,LABEL_np)\n",
    "\n",
    "    \n",
    "    print(EDA_np.shape)\n",
    "    print(BVP_np.shape)\n",
    "    print(ACC_X_np.shape)\n",
    "    print(ACC_Y_np.shape)\n",
    "    print(ACC_Z_np.shape)\n",
    "    print(TEM_np.shape)\n",
    "    print(LABEL_np.shape)\n",
    "    \n",
    "\n",
    "    EDA = EDA_np.flatten().tolist()\n",
    "    BVP = BVP_np.flatten().tolist()\n",
    "    ACC_X = ACC_X_np.flatten().tolist()\n",
    "    ACC_Y = ACC_Y_np.flatten().tolist()\n",
    "    ACC_Z = ACC_Z_np.flatten().tolist()\n",
    "    TEM = TEM_np.flatten().tolist()\n",
    "#     print(len(EDA))\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_set = np.array([EDA, BVP, ACC_X, ACC_Y, ACC_Z, TEM],dtype=np.float64)\n",
    "    train_label = np.array([LABEL_np], dtype=np.float64)\n",
    "    \n",
    "\n",
    "    # Reshape the tensor train set, counting the number of rows dividing by 2400\n",
    "    uniform_len = max(len(EDA),len(BVP),len(ACC_X),len(ACC_Y),len(ACC_Z),len(TEM))    \n",
    "    row_training_set = uniform_len // 2400\n",
    "    if (uniform_len % 2400) != 0:\n",
    "        return \"Error in missing_zeros\"\n",
    "    train_set = train_set.reshape(6,row_training_set,2400)\n",
    "\n",
    "    print(train_set.shape)\n",
    "    train_set = np.dstack(train_set)\n",
    "    print(train_label.shape)\n",
    "    train_label = train_label.reshape(train_label.shape[1], 1)\n",
    "    \n",
    "    print(train_label.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"END read data\")\n",
    "    return train_set, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "27e11234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(start_session, end_session):\n",
    "#     create list of folders\n",
    "    listt = os.listdir(\"../Sessions/\")\n",
    "\n",
    "#   how many users? for now only 1\n",
    "    list_dir = sorted(listt)[0:1]\n",
    "#     list_dir = sorted(listt)\n",
    "    list_path_dir = []\n",
    "    for elem in list_dir:\n",
    "        list_path_dir.append(\"../Sessions/\" + elem)\n",
    "\n",
    "#     read files in the folder and create a list of files \n",
    "    list_ordered_files = []\n",
    "    \n",
    "    for folder in list_path_dir:\n",
    "#         print(folder)\n",
    "        list_files = []\n",
    "        for infile in os.listdir(folder):\n",
    "            list_files.append(infile)\n",
    "        list_files = sorted(list_files)\n",
    "#         print(list_files)\n",
    "\n",
    "    #     divide the list into 4 sections: acc, bvp, eda, temp\n",
    "    #     take one from each and construct a list \n",
    "    #     the order is really important while reading files\n",
    "        \n",
    "        acc = []\n",
    "        bvp = []\n",
    "        eda = []\n",
    "        tem = []\n",
    "        for elem in (list_files):\n",
    "            if elem[:3] == \"ACC\":\n",
    "                acc.append(elem)\n",
    "            elif elem[:3] == \"BVP\":\n",
    "                bvp.append(elem)\n",
    "            elif elem[:3] == \"EDA\":\n",
    "                eda.append(elem)\n",
    "            elif elem[:3] == \"TEM\":\n",
    "                tem.append(elem)\n",
    "\n",
    "        acc = acc[start_session:end_session]\n",
    "        bvp = bvp[start_session:end_session]\n",
    "        eda = eda[start_session:end_session]\n",
    "        tem = tem[start_session:end_session]\n",
    "    #     create a list with that order: [acc1, bvp1, eda1, tem1, acc2, bvp2, eda2, tem2,...]\n",
    "        for i in range(len(acc)):\n",
    "            list_ordered_files.append(folder + \"/\" + acc[i])\n",
    "            list_ordered_files.append(folder + \"/\" + bvp[i])\n",
    "            list_ordered_files.append(folder + \"/\" + eda[i])\n",
    "            list_ordered_files.append(folder + \"/\" + tem[i])\n",
    "#     print(list_ordered_files)\n",
    "        \n",
    "    \n",
    "    return read_data(list_ordered_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "28eb7e58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266, 2400)\n",
      "(266, 2400)\n",
      "(266, 2400)\n",
      "(266, 2400)\n",
      "(266, 2400)\n",
      "(266, 2400)\n",
      "(266,)\n",
      "(6, 266, 2400)\n",
      "(1, 266)\n",
      "(266, 1)\n",
      "END read data\n"
     ]
    }
   ],
   "source": [
    "# 4 is the number of sessions per user\n",
    "# train_set, train_label = create_datasets(0,4)\n",
    "# from 0 session to 5 session per user\n",
    "train_set, train_label =  create_datasets(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "de58c551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 2400)\n",
      "(49, 2400)\n",
      "(49, 2400)\n",
      "(49, 2400)\n",
      "(49, 2400)\n",
      "(49, 2400)\n",
      "(49,)\n",
      "(6, 49, 2400)\n",
      "(1, 49)\n",
      "(49, 1)\n",
      "END read data\n"
     ]
    }
   ],
   "source": [
    "test_set, test_label = create_datasets(5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b4571c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_model(train_set, train_label) :\n",
    "    \n",
    "    opt = Adam(learning_rate=0.01,beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name=\"Adam\")\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(2400, 6), name=\"layer1\"))\n",
    "    model.add(Dense(12, activation='relu', name=\"layer2\"))\n",
    "    model.add(Dense(1, activation='sigmoid', name=\"layer3\"))\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(np.array(train_set), np.array(train_label), epochs=10, batch_size=128)\n",
    "    print(\"END network model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7c2e4227",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 41ms/step - loss: 4.8059 - accuracy: 0.4557\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 2.1405 - accuracy: 0.6838\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 1.6232 - accuracy: 0.6639\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 1.2896 - accuracy: 0.6584\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.9938 - accuracy: 0.7057\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7723 - accuracy: 0.7031\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.7110 - accuracy: 0.6151\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6036 - accuracy: 0.6986\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6757 - accuracy: 0.7221\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.6132 - accuracy: 0.7117\n",
      "END network model\n"
     ]
    }
   ],
   "source": [
    "network_model(train_set,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "094b7adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3)\n",
      "(7, 3)\n",
      "[ True  True  True  True  True  True False]\n",
      "[[0. 0. 0.]]\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "a = [[[np.nan,np.nan,np.nan],[np.nan,np.nan,np.nan],[np.nan,np.nan,np.nan]], [[np.nan,np.nan,np.nan], [np.nan,np.nan,np.nan], [np.nan,np.nan,np.nan]]]\n",
    "a = np.array(a)\n",
    "print(a.shape)\n",
    "a = a.reshape(1, 6, 3)\n",
    "a = a[0]\n",
    "# print(a)\n",
    "\n",
    "a = np.concatenate((a, [[0,0,0]]), axis=0)\n",
    "# print(a)\n",
    "print(a.shape)\n",
    "\n",
    "array_boolean = np.isnan(a).any(axis=1)\n",
    "\n",
    "print(array_boolean)\n",
    "# iterate over each row.\n",
    "# if delete a \"true\" then continue in a while untill it goes 1 step forward\n",
    "i = 0\n",
    "while i < len(array_boolean):\n",
    "    while array_boolean[i]:\n",
    "        a = np.delete(a, i, 0)\n",
    "        array_boolean = np.delete(array_boolean, i)\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "print(a)\n",
    "print(array_boolean)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1,2,3,4,5,6,7,8]\n",
    "B = [1,2,3,4,5,6,7,8]\n",
    "lis = [A, B]\n",
    "lis = np.array(lis)\n",
    "for elem in lis:\n",
    "    print(elem)\n",
    "    elem = np.array(elem)\n",
    "    print(elem)\n",
    "\n",
    "print(lis[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae900d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([1,2,3,4])\n",
    "A = A[:-1]\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "id": "8db82c8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LABEL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-936-28baaa1794c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLABEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'LABEL' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "91b17f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6246 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8927670d0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDA = np.array([[111,112,113,114,115,116,117], [121,122,123,124,125,126,127]])\n",
    "BVP = np.array([[211,212,213,214,215,216,217], [221,222,223,224,225,226,227]])\n",
    "ACC = np.array([[311,312,313,314,315,316,317], [321,322,323,324,325,326,327]])\n",
    "\n",
    "train_data = np.array([EDA, BVP, ACC])\n",
    "\n",
    "train_data_labels = np.array([[0], [1]])\n",
    "\n",
    "train_data.shape  # (3, 2, 7)\n",
    "\n",
    "train_data = np.dstack(train_data)\n",
    "\n",
    "train_data.shape  # (2, 7, 3)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(16, activation='relu',input_shape=(7, 3)))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_data_labels, epochs=15, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d0852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676abe3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
